{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFTO9GPXjE1E"
      },
      "source": [
        "# **Installing the required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VrsJ1gbeSGG"
      },
      "outputs": [],
      "source": [
        "!pip  install -U farasapy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm73M_mLjWZX"
      },
      "source": [
        "# **Importing the required Libraries and Tools**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt2YujmmjYyg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import torch\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support,  classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from farasa.stemmer import FarasaStemmer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import time\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQYtVIycjcUn"
      },
      "source": [
        "## **Data Preparation for Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SHCWUvq7JUq"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Defining the data file paths in Google Drive for Train and Test splits of the First Scenraio\n",
        "drive_file_path_train = \"/content/drive/MyDrive/#####\"\n",
        "drive_file_path_test = \"/content/drive/MyDrive/#####\"\n",
        "\n",
        "\n",
        "# Reading the CSV file into a DataFrame\n",
        "train_data = pd.read_csv(drive_file_path_train)\n",
        "test_data = pd.read_csv(drive_file_path_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyDzFpzg6Wkd"
      },
      "outputs": [],
      "source": [
        "# Defining the data file path in Google Drive for Train and Test splits of the Second Scenraio\n",
        "drive_file_path_cx_train = \"/content/drive/MyDrive/#####\"\n",
        "drive_file_path_cx_test = \"/content/drive/MyDrive/#####\"\n",
        "\n",
        "\n",
        "# Reading the CSV file into a DataFrame\n",
        "cx_train_data = pd.read_csv(drive_file_path_cx_train)\n",
        "cx_test_data = pd.read_csv(drive_file_path_cx_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x15GOnd_kWmo"
      },
      "source": [
        "***Utilizing GPU Capabilities***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFTx0Q5XkUJW"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV2AGUPWlnuA"
      },
      "source": [
        "***Further Preprocessing Steps***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR6jdv7Jm8tF"
      },
      "source": [
        "***Applying Stemming using Farasa Stemmer***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOZ_937wm77z"
      },
      "outputs": [],
      "source": [
        "# Initializing Farasa Stemmer\n",
        "farasa_stemmer = FarasaStemmer(interactive=True)\n",
        "\n",
        "# Function to  stem text\n",
        "def stem_text(text):\n",
        "\n",
        "\n",
        "    # Stemming the segmented text\n",
        "    stemmed_text = ' '.join(farasa_stemmer.stem(text).split())\n",
        "\n",
        "\n",
        "    return stemmed_text\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "train_data['Processed_Content'] = train_data['Processed_Content'].apply(stem_text)\n",
        "test_data['Processed_Content'] = test_data['Processed_Content'].apply(stem_text)\n",
        "\n",
        "cx_train_data['Processed_Content'] = cx_train_data['Processed_Content'].apply(stem_text)\n",
        "cx_test_data['Processed_Content'] = cx_test_data['Processed_Content'].apply(stem_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FphdQoplhiUc"
      },
      "source": [
        "***Applying Label Encoding and Vectorization through TF-IDF Vectorizer***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LilSyFtmCKdn"
      },
      "outputs": [],
      "source": [
        "# Label Encoding fro the Sector column in train and test splits of the First Scenario\n",
        "\n",
        "# Initializing the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels\n",
        "train_data['Sector'] = label_encoder.fit_transform(train_data['Sector'])\n",
        "test_data['Sector'] = label_encoder.fit_transform(test_data['Sector'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37TvtERHz3Th"
      },
      "outputs": [],
      "source": [
        "# Label Encoding fro the Sector column in train and test splits of the Second Scenario\n",
        "\n",
        "# Initializing the label encoder\n",
        "label_encoder_cx = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels\n",
        "cx_train_data['Sector'] = label_encoder_cx.fit_transform(cx_train_data['Sector'])\n",
        "cx_test_data['Sector'] = label_encoder_cx.fit_transform(cx_test_data['Sector'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXGAGjFimlx"
      },
      "outputs": [],
      "source": [
        "# Get the class names\n",
        "class_names = label_encoder.classes_\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nL-jPa2Yk_q"
      },
      "outputs": [],
      "source": [
        "# Defining the Arabic-to-English translation dictionary which corresponds to the class labels (given that label encoding follows the alphabetical order)\n",
        "translations = {\n",
        "    'الاتصالات': 'Communication',\n",
        "    'البنوك': 'Banking',\n",
        "    'البيئة': 'Environment',\n",
        "    'التعليم': 'Education',\n",
        "    'التموين': 'Supply',\n",
        "    'الزراعة': 'Agriculture',\n",
        "    'الصحة': 'Healthcare',\n",
        "    'القضاء': 'Judiciary',\n",
        "    'الكهرباء': 'Electricity',\n",
        "    'المياه والصرف الصحي': 'Water and Sanitation'\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0bKGqy1rBSh"
      },
      "outputs": [],
      "source": [
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Applying TF-IDF in train and test splits of the First Scenario\n",
        "X_train_tfidf = vectorizer.fit_transform(train_data.loc[:,'Processed_Content'])\n",
        "X_test_tfidf = vectorizer.transform(test_data.loc[:,'Processed_Content'])\n",
        "\n",
        "# Generating the y_train and y-test of the First Scenario\n",
        "y_train = train_data.loc[:,'Sector']\n",
        "y_test = test_data.loc[:,'Sector']\n",
        "\n",
        "# Applying TF-IDF in train and test splits of the Second Scenario\n",
        "X_cx_train_tfidf = vectorizer.fit_transform(cx_train_data.loc[:,'Processed_Content'])\n",
        "X_cx_test_tfidf = vectorizer.transform(cx_test_data.loc[:,'Processed_Content'])\n",
        "\n",
        "\n",
        "# Generating the y_train and y-test of the Second Scenario\n",
        "y_cx_train = cx_train_data.loc[:,'Sector']\n",
        "y_cx_test = cx_test_data.loc[:,'Sector']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpr24QK9smsg"
      },
      "outputs": [],
      "source": [
        "def class_weights_calculator(y_train):\n",
        "\n",
        "    # Calculate class weights based on the training labels\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "    # Define the SVM model with class weights\n",
        "    svm = SVC(class_weight=class_weights_dict, random_state=1)\n",
        "\n",
        "    return svm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsqMGpLPtFBn"
      },
      "source": [
        "## **SVM Model Validation for the Both Scenarios**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avqupIfduhIF"
      },
      "source": [
        "***Using GridSearchCV to find the Best Hyperparamters and provide Model Validation Results based on the chosen best ones. GridSearchCV is used with 10-Fold Cross Validation***\n",
        "\n",
        "***The model validation function should be executed independently for each scenario, using its own data splits***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyHCJrv9hUzw"
      },
      "outputs": [],
      "source": [
        "def validation(svm_model, param_grid, X_train_tfidf, y_train, scenario):\n",
        "\n",
        "  # svm_model is the ones created from the class_weights_calculator function\n",
        "\n",
        "  # Defining the parameter grid for GridSearch\n",
        "  param_grid = param_grid # Actual Parameters Search Space should be added\n",
        "\n",
        "  # Perform GridSearch with 10-folds cross-validation using the svm_model returned from the class_weights_calculator\n",
        "  grid_search = GridSearchCV(svm_model, param_grid, cv=10, scoring='f1_macro', n_jobs=-1)\n",
        "  grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "  # Get the best model from GridSearch\n",
        "  best_svm = grid_search.best_estimator_\n",
        "\n",
        "  if scenario == 1:\n",
        "    print(\"1st Experimental Scenario\\n\")\n",
        "  else :\n",
        "    print(\"2nd Experimental Scenario\\n\")\n",
        "\n",
        "  print(\"Best Parameters:\", grid_search.best_params_)  # Print the best parameters\n",
        "\n",
        "  # Get the best cross-validation score (validation score) for the best parameters\n",
        "  print(\"Best Cross-Validation F1-Score (Macro):\", grid_search.best_score_)\n",
        "\n",
        "  return best_svm, grid_search.best_params_, grid_search.best_score_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gITTzpeHxnZS"
      },
      "source": [
        "## **SVM Model Evaluation for the Both Scenarios**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKKR0YkRxreV"
      },
      "source": [
        "***The model evaluation function should be executed independently for each scenario, using its own data splits***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN7Efppjhh9y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(best_svm, X_train_tfidf, X_test_tfidf, y_train, y_test, translations_dict, scenario):\n",
        "\n",
        "  # Start the timer for training\n",
        "  start_time = time.time()\n",
        "\n",
        "  best_svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "  # Evaluate on the test set\n",
        "  y_test_pred = best_svm.predict(X_test_tfidf)\n",
        "\n",
        "  # End the timer for evaluation\n",
        "  end_time = time.time()\n",
        "\n",
        "  train_eval_time = end_time - start_time\n",
        "  print(f\"Training and Evaluation Time: {train_eval_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "  # labels and their corresponding names\n",
        "  label_names = [f\"{j}\" for i, j in translations_dict.items()]\n",
        "\n",
        "  # Generate the classification report with label names\n",
        "  eval_classification_report = classification_report(y_test, y_test_pred, target_names=label_names)\n",
        "\n",
        "  if scenario == 1:\n",
        "    print(\"Evaluation Classification Report the 1st Experimental Scenario:\\n\\n\", eval_classification_report,\"\\n\")\n",
        "  else :\n",
        "    print(\"Evaluation Classification Report the 2nd Experimental Scenario:\\n\\n\", eval_classification_report,\"\\n\")\n",
        "\n",
        "  # Printing evaluation metrics:\n",
        "  # Macro average\n",
        "  macro_f1_score = f1_score(y_test, y_test_pred, average='macro')\n",
        "  print(\"F1_score_Macro:\", macro_f1_score, \"\\n\")\n",
        "\n",
        "  # Weighted average\n",
        "  weighted_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n",
        "  print(\"F1_score_Weighted:\", weighted_f1_score)\n",
        "\n",
        "  return y_test_pred, eval_classification_report, macro_f1_score, weighted_f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Checking the Classification Report***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7lf5TlcC7fd"
      },
      "outputs": [],
      "source": [
        "def classification_report_heatmap(eval_classification_report, scenario):\n",
        "\n",
        "  # Convert the report to a DataFrame\n",
        "  df_report = pd.DataFrame(eval_classification_report).transpose()\n",
        "\n",
        "  # Separate the support column to add it as a separate column without color\n",
        "  support = df_report['support'].iloc[:-1]  # Exclude the last row (accuracy row if needed)\n",
        "\n",
        "  # Remove the support column from the heatmap to avoid coloring it\n",
        "  df_report_no_support = df_report.drop(columns='support').iloc[:-1, :]  # Exclude last row (accuracy)\n",
        "\n",
        "  # Display heatmap\n",
        "  plt.figure(figsize=(4 ,6))\n",
        "  sns.heatmap(df_report.iloc[:, :-1], annot=True, cmap=\"vlag_r\" ,fmt=\".4f\", cbar=True, linewidths=0.5 )\n",
        "\n",
        "\n",
        "  # Move the x-axis labels (precision, recall, f1-score) to the top\n",
        "  plt.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
        "\n",
        "\n",
        "  if scenario == 1:\n",
        "    plt.title('SVM Classification Report Heatmap (1st Experimental Scenario)\\n', fontsize=12)\n",
        "  else:\n",
        "    plt.title('SVM Classification Report Heatmap (2nd Experimental Scenario)\\n', fontsize=12)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return df_report\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Checking the Confusion Matrix***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSp-CnAozuJ6"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_heatmap(y_test, y_test_pred, translations_dict, scenario):\n",
        "\n",
        "  # Assuming you have y_test (true labels) and y_pred (predicted labels)\n",
        "  # Generate the confusion matrix\n",
        "  eval_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "  # labels and their corresponding names\n",
        "  label_names = [f\"{j}\" for i, j in translations_dict.items()]\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(7, 5))\n",
        "\n",
        "\n",
        "  # Add labels, title, and adjust the plot\n",
        "  # Plot confusion matrix with the \"Blues\" color map\n",
        "  sns.heatmap(eval_conf_matrix, annot=True, cmap=\"Blues\", fmt=\"g\", xticklabels=label_names, yticklabels=label_names, cbar=True, linewidths=0.5)\n",
        "\n",
        "\n",
        "  plt.xlabel('Predicted Labels', fontsize=12)\n",
        "  plt.ylabel('True Labels', fontsize=12)\n",
        "\n",
        "  if scenario == 1:\n",
        "    plt.title('SVM Confusion Matrix Heatmap (1st Experimental Scenario)\\n', fontsize=12)\n",
        "  else:\n",
        "    plt.title('SVM Confusion Matrix Heatmap (2nd Experimental Scenario)\\n', fontsize=12)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return eval_conf_matrix\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
