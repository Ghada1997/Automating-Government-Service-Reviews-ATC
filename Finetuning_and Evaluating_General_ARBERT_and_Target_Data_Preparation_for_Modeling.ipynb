{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFTO9GPXjE1E"
      },
      "source": [
        "# **Installing the required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkKPOowFi4Ez"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install optuna\n",
        "!pip install accelerate\n",
        "!pip install transformers datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm73M_mLjWZX"
      },
      "source": [
        "# **Importing the required Libraries and Tools**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt2YujmmjYyg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import transformers\n",
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support,  classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "from transformers.trainer_utils import set_seed\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainerCallback, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "from google.colab import drive\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQYtVIycjcUn"
      },
      "source": [
        "## **Data Preparation for Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubyeDy9JXczm"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Defining the data file path in Google Drive for the Train and Test splits of the First Scenario\n",
        "train_data_path = '/content/drive/My Drive/#####'\n",
        "test_data_path = '/content/drive/My Drive/#####'\n",
        "\n",
        "# Reading the CSV file into a DataFrame\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56UfE4iZ14Hn"
      },
      "outputs": [],
      "source": [
        "# Defining the data file path in Google Drive for the Train and Test splits of the Second Scenario\n",
        "cx_train_data_path = '/content/drive/My Drive/#####'\n",
        "cx_test_data_path = '/content/drive/My Drive/#####'\n",
        "\n",
        "# Reading the CSV file into a DataFrame\n",
        "cx_train_data = pd.read_csv(cx_train_data_path)\n",
        "cx_test_data = pd.read_csv(cx_test_data_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x15GOnd_kWmo"
      },
      "source": [
        "***Utilizing GPU Capabilities***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFTx0Q5XkUJW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Checking if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. Running on GPU.\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Running on CPU.\")\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV2AGUPWlnuA"
      },
      "source": [
        "***Adopting ARBERT Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OzniL7_lSs6"
      },
      "outputs": [],
      "source": [
        "# Defining tokenizer\n",
        "\n",
        "ARBERT_tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/ARBERT\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDtan5KGoMfE"
      },
      "source": [
        "### **Preparing the data to apply the fine-tuning for general ARBERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc1Gd-5tr72J"
      },
      "outputs": [],
      "source": [
        "# Renaming Columns to prepare it for modeling\n",
        "\n",
        "train_data.rename(columns={'Sector': 'labels',  'Processed_Content':'text'}, inplace=True)\n",
        "test_data.rename(columns={'Sector': 'labels',  'Processed_Content':'text'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdrDEYg4XrVh"
      },
      "outputs": [],
      "source": [
        "cx_train_data.rename(columns={'Sector': 'labels',  'Processed_Content':'text'}, inplace=True)\n",
        "cx_test_data.rename(columns={'Sector': 'labels',  'Processed_Content':'text'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcxid8zgmNEI"
      },
      "source": [
        "***Some EDA to understand the Token (Review) length distribution in the data which will affect the ARBERT max_length and padding properties***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLx2iPFVmgNL"
      },
      "outputs": [],
      "source": [
        "# Visualizing the Distribution of Review Lengths to decide on the max_length\n",
        "\n",
        "# Step 1: Calculate Review Lengths\n",
        "review_lengths = []\n",
        "\n",
        "preprocessed_df = pd.concat([train_data, test_data], ignore_index=True)\n",
        "\n",
        "# Tokenize and calculate length for each review\n",
        "for review_text in preprocessed_df['text']:\n",
        "    # Tokenize the review\n",
        "    tokens = ARBERT_tokenizer.tokenize(review_text)\n",
        "    # Calculate the length (number of tokens) of the review\n",
        "    length = len(tokens)\n",
        "    review_lengths.append(length)\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 2: Summary Statistics\n",
        "mean_length = sum(review_lengths) / len(review_lengths)\n",
        "median_length = sorted(review_lengths)[len(review_lengths) // 2]\n",
        "min_length = min(review_lengths)\n",
        "max_length = max(review_lengths)\n",
        "\n",
        "# Step 3: Visualize the Distribution\n",
        "plt.figure(figsize=(9, 5))\n",
        "\n",
        "ax = sns.histplot(review_lengths, bins=50, alpha=0.6, color='royalblue', edgecolor='black', kde=True)\n",
        "ax.axvline(mean_length, color='k', linestyle='--') # mean\n",
        "ax.axvline(median_length, color='b', linestyle='--', ) # median\n",
        "\n",
        "plt.title('Distribution of Review Lengths', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Length of Review', fontsize=18)\n",
        "plt.ylabel('Frequency', fontsize=18)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "print(\"Mean Review Length:\", mean_length)\n",
        "print(\"Median Review Length:\", median_length)\n",
        "print(\"Minimum Review Length:\", min_length)\n",
        "print(\"Maximum Review Length:\", max_length)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag65EplOHG_p"
      },
      "source": [
        "***Applying Label Encoding***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gop01ETR2e1R"
      },
      "outputs": [],
      "source": [
        "# Label Encoding 'labels' column to be prepared for the modeling in the First Scenario\n",
        "\n",
        "# Initializing the label encoder\n",
        "label_encoder_train = LabelEncoder()\n",
        "label_encoder_test = LabelEncoder()\n",
        "\n",
        "# Fitting and transforming the labels to integers\n",
        "train_data['labels'] = label_encoder_train.fit_transform(train_data['labels'])\n",
        "test_data['labels'] = label_encoder_test.fit_transform(test_data['labels'])\n",
        "\n",
        "# Saving the label map for later use (e.g., interpreting model predictions)\n",
        "label_map_train = dict(zip(label_encoder_train.classes_, label_encoder_train.transform(label_encoder_train.classes_)))\n",
        "label_map_test = dict(zip(label_encoder_test.classes_, label_encoder_test.transform(label_encoder_test.classes_)))\n",
        "\n",
        "# Ensure labels are integers\n",
        "train_data['labels'] = train_data['labels'].astype(int)\n",
        "test_data['labels'] = test_data['labels'].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yePHlYHX3-r"
      },
      "outputs": [],
      "source": [
        "# Label Encoding 'labels' column to be prepared for the modeling in the Second Scenario\n",
        "\n",
        "# Initializing the label encoder\n",
        "label_encoder_cx_train = LabelEncoder()\n",
        "label_encoder_cx_test = LabelEncoder()\n",
        "\n",
        "# Fitting and transforming the labels to integers\n",
        "cx_train_data['labels'] = label_encoder_cx_train.fit_transform(cx_train_data['labels'])\n",
        "cx_test_data['labels'] = label_encoder_cx_test.fit_transform(cx_test_data['labels'])\n",
        "\n",
        "# Saving the label map for later use (e.g., interpreting model predictions)\n",
        "label_map_cx_train = dict(zip(label_encoder_cx_train.classes_, label_encoder_cx_train.transform(label_encoder_cx_train.classes_)))\n",
        "label_map_cx_test = dict(zip(label_encoder_cx_test.classes_, label_encoder_cx_test.transform(label_encoder_cx_test.classes_)))\n",
        "\n",
        "# Ensure labels are integers\n",
        "cx_train_data['labels'] = cx_train_data['labels'].astype(int)\n",
        "cx_test_data['labels'] = cx_test_data['labels'].astype(int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64ElVPdWqNaR"
      },
      "source": [
        "***Preparing Train and Eval Datasets for performing Training, Validation, and Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvYFo7ARI44I"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples ): # Setting the max_length to 512 based on the previous Review Length Distribution\n",
        "\n",
        "    return ARBERT_tokenizer.batch_encode_plus(examples['text'],\n",
        "                                              return_attention_mask=True,\n",
        "                                              padding='max_length',  # Pad to the maximum sequence length\n",
        "                                              truncation=True,  # Truncate sequences longer than the maximum sequence length\n",
        "                                              return_tensors='pt',  # Return PyTorch tensors\n",
        "                                              max_length=512,  # Adjust max_length as needed\n",
        "                                              add_special_tokens=True, # Add special tokens\n",
        "                                              return_special_tokens_mask=True)\n",
        "\n",
        "\n",
        "def prepare_data(train_data_split, test_data_split):\n",
        "\n",
        "\n",
        "  # Converting the split DataFrames to Dataset objects\n",
        "  train_dataset_mlm = Dataset.from_pandas(train_data_split)\n",
        "  eval_dataset_mlm = Dataset.from_pandas(test_data_split)\n",
        "\n",
        "\n",
        "  # Tokenize the train and eval datasets\n",
        "  tokenized_train_dataset_mlm = train_dataset_mlm.map(tokenize_function, batched=True, remove_columns=['']) # Remove extra columns as needed\n",
        "  tokenized_eval_dataset_mlm = eval_dataset_mlm.map(tokenize_function, batched=True, remove_columns=['']) # Remove extra columns as needed\n",
        "\n",
        "  # Remove the unnecessary column '__index_level_0__' if present\n",
        "  if '__index_level_0__' in tokenized_train_dataset_mlm.column_names:\n",
        "     tokenized_train_dataset_mlm = tokenized_train_dataset_mlm.remove_columns(['__index_level_0__'])\n",
        "  if '__index_level_0__' in tokenized_eval_dataset_mlm.column_names:\n",
        "     tokenized_eval_dataset_mlm = tokenized_eval_dataset_mlm.remove_columns(['__index_level_0__'])\n",
        "\n",
        "\n",
        "  return tokenized_train_dataset_mlm, tokenized_eval_dataset_mlm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRIeYUmcMQS4"
      },
      "outputs": [],
      "source": [
        "# Data collator for dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer = ARBERT_tokenizer, padding = 'max_length', max_length = 512, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMjCwXSgt_81"
      },
      "source": [
        "# **Modeling: Finetuning ARBERT for Classifying Arabic Government Reviews to their relavant Government Sector**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC-WnvM3uAFw"
      },
      "outputs": [],
      "source": [
        "# Initializing logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Custom logging callback\n",
        "class CustomCallback(transformers.TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        # Custom logging every 100 steps\n",
        "        if state.global_step % 100 == 0:\n",
        "            logger.info(f\"Step {state.global_step}: Continuing training...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdTaSsO2kzCa"
      },
      "outputs": [],
      "source": [
        "# Specify output directory in Google Drive for checkpoints\n",
        "output_dir_checkpoints = \"/content/drive/MyDrive/#####\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaEw16hurBP"
      },
      "source": [
        "***Defining Training Arguments***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYWzAFjXjm0X"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KYpRg43fzgY"
      },
      "outputs": [],
      "source": [
        "def class_weights_calculator(tokenized_train_dataset_mlm):\n",
        "  # 'labels' is the column in the training set\n",
        "  y_train = tokenized_train_dataset_mlm['labels']\n",
        "\n",
        "  # Calculate class weights based on training set\n",
        "\n",
        "  class_counts = np.bincount(y_train)\n",
        "  class_weights = 1. / class_counts\n",
        "  class_weights = class_weights / class_weights.sum()  # Normalize to sum to 1\n",
        "  class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "\n",
        "  return class_counts, class_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovNc-4F0R0W7"
      },
      "outputs": [],
      "source": [
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    def __init__(self, early_stopping_patience=3):\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "        self.best_metric = None\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if state.is_world_process_zero:\n",
        "            current_metric = metrics[\"eval_loss\"]\n",
        "            if self.best_metric is None or current_metric > self.best_metric:\n",
        "                self.best_metric = current_metric\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            if self.patience_counter >= self.early_stopping_patience:\n",
        "                control.should_training_stop = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_uT736wsDNW"
      },
      "source": [
        "***Using Optuna to find the Best Hyperparamters and provide Model Validation Results based on the chosen best ones. Optuna is used with 10-Fold Cross Validation***\n",
        "\n",
        "***This step should be done for each Scenario independently, using its own data splits***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBokJVDztV2T"
      },
      "outputs": [],
      "source": [
        "def optuna_hyperparams_tuning_and_validation(tokenized_train_dataset, search_space, data_collator, scenario):\n",
        "\n",
        "# Assuming search_space is an input provided as a dictionary with keys representing the parameter names and values represnting their respective search spaces\n",
        "\n",
        "  def compute_metrics(pred):\n",
        "      labels = pred.label_ids\n",
        "      preds = pred.predictions.argmax(-1)\n",
        "      f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
        "      return {\n",
        "          'f1': f1\n",
        "      }\n",
        "\n",
        "\n",
        "  def objective(trial):\n",
        "\n",
        "      # Suggest hyperparameters\n",
        "      learning_rate = trial.suggest_categorical('learning_rate', search_space['learning_rate'])\n",
        "      batch_size = trial.suggest_categorical('batch_size', search_space['batch_size'])\n",
        "      num_epochs = trial.suggest_int('num_epochs', *search_space['num_epochs'])\n",
        "      warmup_steps = trial.suggest_int('warmup_steps', *search_space['warmup_steps'])\n",
        "\n",
        "      # Define training arguments\n",
        "      training_args = TrainingArguments(\n",
        "          output_dir=output_dir_checkpoints,\n",
        "          overwrite_output_dir=True,\n",
        "          eval_strategy='steps',\n",
        "          learning_rate=learning_rate,\n",
        "          per_device_train_batch_size= batch_size,\n",
        "          per_device_eval_batch_size= batch_size,\n",
        "          num_train_epochs= num_epochs,\n",
        "          warmup_steps=warmup_steps,\n",
        "          weight_decay=0.01,\n",
        "          logging_steps=20,\n",
        "          logging_dir='./logs',\n",
        "          save_steps=200,\n",
        "          save_total_limit=2,\n",
        "          gradient_accumulation_steps=1,\n",
        "          fp16=True,\n",
        "          eval_steps=100,\n",
        "          load_best_model_at_end=True,\n",
        "          metric_for_best_model=\"eval_f1\",\n",
        "          seed=1,\n",
        "          lr_scheduler_type=\"linear\",\n",
        "          report_to=\"none\",  # To disable logging to third-party services\n",
        "      )\n",
        "\n",
        "\n",
        "      # Early stopping callback\n",
        "      early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)  # Adjust patience as needed\n",
        "\n",
        "\n",
        "      # Convert the dataset to a format suitable for StratifiedKFold\n",
        "      labels = tokenized_train_dataset['labels']\n",
        "      skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "\n",
        "      f1s = []\n",
        "\n",
        "\n",
        "      for train_index, val_index in skf.split(np.zeros(len(labels)), labels):\n",
        "          train_split = tokenized_train_dataset.select(train_index)\n",
        "          val_split = tokenized_train_dataset.select(val_index)\n",
        "\n",
        "          # Ensure each split contains all labels and maintains proportionality\n",
        "          train_labels, train_counts = np.unique(train_split['labels'], return_counts=True)\n",
        "          val_labels, val_counts = np.unique(val_split['labels'], return_counts=True)\n",
        "\n",
        "          print(f\"Train labels distribution: {dict(zip(train_labels, train_counts))}\")\n",
        "          print(f\"Val labels distribution: {dict(zip(val_labels, val_counts))}\")\n",
        "\n",
        "          # Initializing the model inside the loop to reset it for each fold\n",
        "          model = AutoModelForSequenceClassification.from_pretrained('UBC-NLP/ARBERT', num_labels=10)  # Path to the general ARBERT model\n",
        "          tokenizer = ARBERT_tokenizer # general ARBERT model previously intialized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # Defining the Trainer\n",
        "          trainer = CustomTrainer(\n",
        "              class_weights = class_weights_calculator(train_split)[1], # extracting the class weights from the function\n",
        "              model=model,\n",
        "              args=training_args,\n",
        "              train_dataset=train_split,\n",
        "              eval_dataset=val_split,\n",
        "              data_collator=data_collator,\n",
        "              tokenizer=tokenizer,\n",
        "              compute_metrics=compute_metrics,\n",
        "              callbacks=[early_stopping_callback]  # Add early stopping callback\n",
        "          )\n",
        "\n",
        "          # Train the model\n",
        "          trainer.train()\n",
        "\n",
        "          # Evaluate the model\n",
        "          eval_result = trainer.evaluate(eval_dataset=val_split)\n",
        "          f1s.append(eval_result['eval_f1'])\n",
        "\n",
        "      mean_f1 = sum(f1s) / len(f1s)\n",
        "\n",
        "      return mean_f1\n",
        "\n",
        "  # Create the Optuna study and optimize\n",
        "  study = optuna.create_study(direction='maximize')\n",
        "  study.optimize(objective, n_trials=30)  # Run 30 trials\n",
        "\n",
        "  print('Best trial for the 1st Experimental Scenario:\\n:', study.best_trial) if scenario == 1 else print('Best trial for the 2nd Experimental Scenario:\\n:', study.best_trial)\n",
        "\n",
        "  return study.best_trial, study.best_trial.number, study.best_trial.value, study.best_trial.params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igLwU9edIDhI"
      },
      "source": [
        "***Validating the ARBERT model again on a seperate Validation set just to be able to assess the model training behavior through generating Training and Validation Losses as well as Validation F1 Scores across Steps***\n",
        "\n",
        "***This step should be done for each Scenario independently, using its own data splits***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dSsmm6tIBEW"
      },
      "outputs": [],
      "source": [
        "def validate_on_a_separate_validation_set(tokenized_train_dataset, params, data_collator, output_dir_checkpoints, scenario):\n",
        "\n",
        "  # Assuming the params are the best parameters generated from optuna trials and provided as an input in the form a dictionary of parameter names and their values\n",
        "\n",
        "  # Also, assuming the dataset is a PyTorch Dataset that has been tokenized.\n",
        "  dataset = tokenized_train_dataset\n",
        "\n",
        "  # Extract labels for stratified sampling\n",
        "  labels = dataset['labels']\n",
        "\n",
        "  # Split the dataset into training and validation sets using stratified sampling\n",
        "  train_idx, val_idx = train_test_split(\n",
        "      np.arange(len(labels)),  # Indices of the dataset\n",
        "      test_size=0.2,  # 20% for validation\n",
        "      stratify=labels,  # Stratify based on labels\n",
        "      random_state=2\n",
        "  )\n",
        "\n",
        "  # Create train and validation subsets\n",
        "  train_subset = Subset(dataset, train_idx)\n",
        "  val_subset = Subset(dataset, val_idx)\n",
        "\n",
        "\n",
        "  def compute_metrics(p):\n",
        "      predictions, labels = p\n",
        "      predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "      # Calculate accuracy\n",
        "      accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "      precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
        "      precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)\n",
        "\n",
        "      report = classification_report(labels, predictions, output_dict=True)\n",
        "\n",
        "      return {\n",
        "\n",
        "          'f1_macro': f1_macro\n",
        "\n",
        "            }\n",
        "\n",
        "  final_output_dir = output_dir_checkpoints\n",
        "\n",
        "\n",
        "  # Final training with the best hyperparameters\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=final_output_dir,\n",
        "      overwrite_output_dir=True,\n",
        "      eval_strategy='steps',\n",
        "      learning_rate= params['learning_rate'],\n",
        "      per_device_train_batch_size= params['batch_size'],\n",
        "      per_device_eval_batch_size= params['batch_size'],\n",
        "      num_train_epochs= params['num_epochs'],\n",
        "      weight_decay=0.01,\n",
        "      warmup_steps= params['warmup_steps'],\n",
        "      gradient_accumulation_steps=1,\n",
        "      logging_steps=50,\n",
        "      logging_dir='./logs',\n",
        "      save_steps=200,\n",
        "      save_total_limit=2,\n",
        "      fp16=True,\n",
        "      eval_steps=50,\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model = \"f1_macro\",\n",
        "      seed=1,\n",
        "      lr_scheduler_type=\"linear\",\n",
        "      report_to=\"none\"\n",
        "  )\n",
        "\n",
        "\n",
        "  early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "\n",
        "\n",
        "  # Create train and validation subsets\n",
        "  train_subset = Subset(dataset, train_idx)\n",
        "  val_subset = Subset(dataset, val_idx)\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained('UBC-NLP/ARBERT', num_labels=10)\n",
        "\n",
        "  # Apply .contiguous() before saving or checkpointing\n",
        "  for param in model.parameters():\n",
        "      param.data = param.data.contiguous()\n",
        "\n",
        "  # Now save the model or checkpoint\n",
        "  model.save_pretrained(final_output_dir)\n",
        "\n",
        "\n",
        "\n",
        "  # Update the Trainer with the new data\n",
        "  trainer = CustomTrainer(\n",
        "          class_weights=class_weights_calculator(train_subset)[1],\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          train_dataset=train_subset,\n",
        "          eval_dataset=val_subset,\n",
        "          data_collator=data_collator,\n",
        "          tokenizer=ARBERT_tokenizer,\n",
        "          compute_metrics=compute_metrics,\n",
        "          callbacks=[early_stopping]\n",
        "      )\n",
        "\n",
        "  # Train the model on the current fold\n",
        "  trainer.train()\n",
        "\n",
        "  # Evaluate the model on the validation set of the current fold\n",
        "  eval_result = trainer.evaluate(eval_dataset=val_subset)\n",
        "\n",
        "\n",
        "  # Print the averaged results\n",
        "  print(\"Final Validation Results for the 1st Experimental Scenario:\") if scenario == 1 else print(\"Final Validation Results for the 2nd Experimental Scenario:\")\n",
        "  print(\"--------------------------------\\n\")\n",
        "  for metric, value in eval_result.items():\n",
        "      print(f\"{metric}: {value}\")\n",
        "      print(\"--------------------------------\")\n",
        "\n",
        "  return eval_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUql0eAoIBEX"
      },
      "outputs": [],
      "source": [
        "def plot_training_and_validation_curves(logs, scenario):\n",
        "\n",
        "  # Assuming these logs are obtained from the validate_on_a_separate_validation_set after training and stored in the logs dictionary passed to this function as an input.\n",
        "  # This is an example of how it looks like:\n",
        "\n",
        "  # logs = {\n",
        "  #     \"steps\": [],  # steps\n",
        "  #     \"train_loss\": [], # the logged training loss values\n",
        "  #     \"val_loss\": [],  # the logged validation loss values\n",
        "  #     \"val_f1\": []  # the logged validation F1 score values\n",
        "  # }\n",
        "\n",
        "\n",
        "  # 1. Plot the training and validation loss curve\n",
        "  plt.figure(figsize=(10, 5))\n",
        "\n",
        "  # Plot training loss\n",
        "  plt.plot(logs[\"steps\"], logs[\"train_loss\"], label=\"Training Loss\", marker='o')\n",
        "\n",
        "  # Plot validation loss\n",
        "  plt.plot(logs[\"steps\"], logs[\"val_loss\"], label=\"Validation Loss\", marker='o')\n",
        "\n",
        "  # Labels and title\n",
        "  plt.xlabel(\"Steps\", fontsize = 14)\n",
        "  plt.ylabel(\"Loss\", fontsize = 14)\n",
        "  plt.title(\"General ARBERT Training and Validation Loss Over Steps (1st Experimental Scenario)\", fontsize = 16) if scenario == 1 else plt.title(\"General ARBERT Training and Validation Loss Over Steps (2nd Experimental Scenario)\", fontsize = 16)\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  # 2. Plot the validation F1 score curve\n",
        "  plt.figure(figsize=(10, 5))\n",
        "\n",
        "  # Plot validation F1 Macro score\n",
        "  plt.plot(logs[\"steps\"], logs[\"val_f1\"], label=\"Validation F1 Macro\", color=\"green\", marker='o')\n",
        "\n",
        "  # Labels and title\n",
        "  plt.xlabel(\"Steps\",  fontsize = 14)\n",
        "  plt.ylabel(\"F1 Score\", fontsize = 14)\n",
        "  plt.title(\"General ARBERT Validation F1 Macro Over Steps (1st Experimental Scenario)\", fontsize = 16) if scenario == 1 else plt.title(\"General ARBERT Validation F1 Macro Over Steps (2nd Experimental Scenario)\", fontsize = 16)\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoHZoSj6aE_D"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uczVaIywAKJK"
      },
      "source": [
        "***This step should be done for each Scenario independently, using its own data splits***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyosqhZIaY8t"
      },
      "outputs": [],
      "source": [
        "# Manually define the Arabic-to-English translation dictionary\n",
        "translations = {\n",
        "    'الاتصالات': 'Communication',\n",
        "    'البنوك': 'Banking',\n",
        "    'البيئة': 'Environment',\n",
        "    'التعليم': 'Education',\n",
        "    'التموين': 'Supply',\n",
        "    'الزراعة': 'Agriculture',\n",
        "    'الصحة': 'Healthcare',\n",
        "    'القضاء': 'Judiciary',\n",
        "    'الكهرباء': 'Electricity',\n",
        "    'المياه والصرف الصحي': 'Water and Sanitation'\n",
        "\n",
        "}\n",
        "\n",
        "label_names = [f\"{j}\" for i, j in translations.items()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r66fmv0aEUn"
      },
      "outputs": [],
      "source": [
        "final_output_dir = \"/content/drive/MyDrive/#####\"\n",
        "\n",
        "def evaluate(tokenized_train_dataset, tokenized_test_dataset, params, label_names, data_collator, final_output_dir, scenario):\n",
        "\n",
        "  # Assuming that the params are generated after applying optuna and given as an input in the form of a dictionary with keys referring to paramter names and values corresponsding to their respective values\n",
        "\n",
        "\n",
        "  def compute_metrics(p):\n",
        "      predictions, labels = p\n",
        "      predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "      precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
        "      precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)\n",
        "\n",
        "      report = classification_report(labels, predictions, output_dict=True, target_names=label_names)\n",
        "\n",
        "      # Compute the confusion matrix\n",
        "      conf_matrix = confusion_matrix(labels, predictions)\n",
        "\n",
        "      return {\n",
        "          'classification_report': report,\n",
        "          'conf_matrix': conf_matrix,\n",
        "          'f1_macro': f1_macro,\n",
        "          'f1_weighted': f1_weighted\n",
        "              }\n",
        "\n",
        "  # Final training with the best hyperparameters\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=final_output_dir,\n",
        "      overwrite_output_dir=True,\n",
        "      eval_strategy='no',\n",
        "      learning_rate= params['learning_rate'],\n",
        "      per_device_train_batch_size= params['batch_size'],\n",
        "      per_device_eval_batch_size= params['batch_size'],\n",
        "      num_train_epochs= params['num_epochs'],\n",
        "      weight_decay=0.01,\n",
        "      warmup_steps= params['warmup_steps'],\n",
        "      gradient_accumulation_steps=1,\n",
        "      logging_steps=50,\n",
        "      logging_dir='./logs',\n",
        "      save_strategy='steps',\n",
        "      save_steps=200,\n",
        "      save_total_limit=2,\n",
        "      fp16=True,\n",
        "      seed=1,\n",
        "      lr_scheduler_type=\"linear\",\n",
        "      report_to=\"none\",\n",
        "  )\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained('UBC-NLP/ARBERT', num_labels=10)\n",
        "\n",
        "  # Apply .contiguous() before saving or checkpointing\n",
        "  for param in model.parameters():\n",
        "      param.data = param.data.contiguous()\n",
        "\n",
        "\n",
        "  # Now save the model\n",
        "  model.save_pretrained(final_output_dir)\n",
        "\n",
        "  early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "  trainer = CustomTrainer(\n",
        "      class_weights=class_weights_calculator(tokenized_train_dataset)[1],\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset= tokenized_train_dataset ,  # Use the entire dataset (Training + Validation) for final training\n",
        "      data_collator=data_collator,\n",
        "      tokenizer=ARBERT_tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "\n",
        "      # Early stopping callback\n",
        "      callbacks=[early_stopping]\n",
        "\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "\n",
        "  # Evaluate the final model\n",
        "  final_eval_result = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
        "\n",
        "  print('Final Evaluation Results of the 1st Experimental Scenario:\\n') if scenario == 1 else print('Final Evaluation Results of the 2nd Experimental Scenario:\\n')\n",
        "\n",
        "\n",
        "  for metric, value in final_eval_result.items():\n",
        "    if metric == 'eval_f1_macro':\n",
        "          print(f\"F1 Score (Macro): {value}\\n\")\n",
        "    elif metric == 'eval_f1_weighted':\n",
        "          print(f\"F1 Score (Weighted): {value}\\n\")\n",
        "    else:\n",
        "      print(f\"{metric}: {value}\\n\")\n",
        "\n",
        "  return final_eval_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57U8NEDpAc_H"
      },
      "source": [
        "***Checking the Classification Report***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW2Tz3ahAa7l"
      },
      "outputs": [],
      "source": [
        "def classification_report_heatmap(eval_classification_report, scenario):\n",
        "  # Assuming eval_classification_report is extract from the evaluate function\n",
        "\n",
        "  # Convert the report to a DataFrame\n",
        "  df_report = pd.DataFrame(eval_classification_report).transpose()\n",
        "\n",
        "  # Separate the support column to add it as a separate column without color\n",
        "  support = df_report['support'].iloc[:-1]  # Exclude the last row (accuracy row if needed)\n",
        "\n",
        "  # Remove the support column from the heatmap to avoid coloring it\n",
        "  df_report_no_support = df_report.drop(columns='support').iloc[:-1, :]  # Exclude last row (accuracy)\n",
        "\n",
        "  # Display heatmap\n",
        "  plt.figure(figsize=(4 ,6))\n",
        "  ax=sns.heatmap(df_report.iloc[:, :-1], annot=True, cmap=\"vlag_r\" ,fmt=\".4f\", cbar=True, linewidths=0.5)\n",
        "\n",
        "  # Overlay the support column on top of the heatmap, without affecting the colors\n",
        "\n",
        "  # Move the x-axis labels (precision, recall, f1-score) to the top\n",
        "  plt.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
        "\n",
        "  plt.title('General ARBERT Classification Report Heatmap (1st Experimental Scenario)\\n', fontsize=12) if scenario == 1 else plt.title('General ARBERT Classification Report Heatmap (2nd Experimental Scenario)\\n', fontsize=12)\n",
        "  plt.show()\n",
        "\n",
        "  pd.options.display.float_format = '{:.4f}'.format\n",
        "\n",
        "  print(\"\\n\", df_report)\n",
        "\n",
        "  return df_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YbZ-uGsCovJ"
      },
      "source": [
        "***Checking the Confusion Matrix***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3e9H-yLCpYz"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_heatmap(eval_conf_matrix, label_names, scenario):\n",
        "  # Assuming the eval_conf_matrix is extracted from the evaluate function\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(7, 5))\n",
        "\n",
        "\n",
        "  # Add labels, title, and adjust the plot\n",
        "  # Plot confusion matrix with the \"Blues\" color map\n",
        "  sns.heatmap(eval_conf_matrix, annot=True, cmap=\"Blues\", fmt=\"g\", xticklabels=label_names, yticklabels=label_names)\n",
        "\n",
        "\n",
        "  plt.xlabel('Predicted Labels', fontsize=12)\n",
        "  plt.ylabel('True Labels', fontsize=12)\n",
        "\n",
        "\n",
        "  plt.title('General ARBERT Confusion Matrix Heatmap (1st Experimental Scenario)\\n', fontsize=14) if scenario == 1 else plt.title('General ARBERT Confusion Matrix Heatmap (2nd Experimental Scenario)\\n', fontsize=14)\n",
        "  plt.show()\n",
        "\n",
        "  return eval_conf_matrix"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
