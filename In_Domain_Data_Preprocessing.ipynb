{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2MQVWW7ZE8V"
      },
      "source": [
        "# **Installing the required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1OZ1saOZkVC"
      },
      "outputs": [],
      "source": [
        "!pip install arabert\n",
        "!pip install nltk\n",
        "!pip install arabic_reshaper\n",
        "!pip install python-bidi\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E0JaJhtZaOa"
      },
      "source": [
        "# **Importing the required Libraries and tool**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LCsbzQKZl8a"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import shutil\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import logging\n",
        "import transformers\n",
        "import arabic_reshaper\n",
        "\n",
        "from transformers import EarlyStoppingCallback, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, DataCollatorForLanguageModeling, AutoModelForMaskedLM\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from bidi.algorithm import get_display\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3nJzrPCFMGL"
      },
      "source": [
        "# **Data Understanding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVshkwL1BA5t"
      },
      "source": [
        "###**Reading the CSV Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdbhaxGIvl0Q"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Defining the path to the directory containing all the CSV files for the Dataset\n",
        "drive_dataset_folder_path = \"/content/drive/MyDrive/########\"\n",
        "\n",
        "# Changing the current working directory\n",
        "os.chdir(drive_dataset_folder_path)\n",
        "\n",
        "# Getting a list of all CSV files in the folder\n",
        "csv_files = glob.glob(\"*.csv\")\n",
        "\n",
        "# Initializing a list to store the DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Looping through the list of CSV files and read each one into a DataFrame\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenating all DataFrames into one for easier analysis\n",
        "merged_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Shuffling the rows of the combined DataFrame\n",
        "PGN_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Displaying the first few rows of the combined DataFrame\n",
        "print(PGN_df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KCnjrLlznHl"
      },
      "source": [
        "### **Data Inspection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY46NBu6BMLZ"
      },
      "source": [
        "***Investigating the Shape of the merged PGN Dataset (Number of Rows and Columns)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40qDEwLuCFBW"
      },
      "outputs": [],
      "source": [
        "# getting number of rows and columns for the dataset\n",
        "\n",
        "print(f\"Number of Rows and Columns in the Datafram: {PGN_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKI3sGP0DKNV"
      },
      "outputs": [],
      "source": [
        "# getting the info of dataframes\n",
        "\n",
        "print(PGN_df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpLQ5mlcDaSM"
      },
      "source": [
        "**Comment:**\n",
        "It was obvious that there were no Null values in any of the dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm7q-SKFBqsk"
      },
      "source": [
        "***Getting a detailed Summary about the dataframe including the Categorical ones***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKmDL3qPD-s8"
      },
      "outputs": [],
      "source": [
        "print(PGN_df.describe(include='all'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-R1mcYb6KyH"
      },
      "source": [
        "***Checking for any null values***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O4NqN6ewTIX"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(PGN_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6neftRuS6SQK"
      },
      "source": [
        "***Checking for any duplicates***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti_FlmMP4jyM"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates in the entire DataFrame\n",
        "duplicates = merged_df.duplicated()\n",
        "\n",
        "# Count the number of duplicate rows\n",
        "num_duplicates = duplicates.sum()\n",
        "\n",
        "# Display the number of duplicates\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVnHs73ZwVD8"
      },
      "outputs": [],
      "source": [
        "# Checking for duplicates in 'Content' column\n",
        "duplicates = PGN_df[PGN_df.duplicated(['Content'], keep=False)]\n",
        "print(\"\\nDuplicate entries in 'Content' column:\")\n",
        "print(duplicates)\n",
        "\n",
        "# Count duplicates in 'Content' column\n",
        "duplicate_count = PGN_df.duplicated(['Content']).sum()\n",
        "print(f\"\\nNumber of duplicate entries in 'Content' column: {duplicate_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XFFAd8C7RLt"
      },
      "outputs": [],
      "source": [
        "# getting unique number of Sectors and their names\n",
        "\n",
        "print(PGN_df['Sector'].nunique())\n",
        "print(PGN_df['Sector'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXlIlB8L5j4N"
      },
      "source": [
        "###**Exploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1BO26W_0nVe"
      },
      "outputs": [],
      "source": [
        "# Dictionary for Arabic to English translations\n",
        "translations = {\n",
        "    'التموين': 'Supply',\n",
        "    'التعليم': 'Education',\n",
        "    'الصحة': 'Healthcare',\n",
        "    'البنوك': 'Banking',\n",
        "    'الاتصالات': 'Communication',\n",
        "    'القضاء': 'Judiciary',\n",
        "    'المياه والصرف الصحي': 'Water and Sanitary',\n",
        "    'الكهرباء': 'Electricity',\n",
        "    'البيئة':'Environment',\n",
        "    'الزراعة': 'Agriculture'\n",
        "\n",
        "}\n",
        "\n",
        "# Counting the number of rows per sector\n",
        "sector_counts = PGN_df['Sector'].value_counts().reset_index()\n",
        "sector_counts.columns = ['Sector', 'Count']\n",
        "\n",
        "# Reversing the letters of each sector name for readability (since they are in Arabic)\n",
        "sector_counts['Reversed_Sector'] = sector_counts['Sector'].apply(lambda x: get_display(arabic_reshaper.reshape(x)))\n",
        "sector_counts['Label'] = sector_counts.apply(lambda x: f\"{x['Reversed_Sector']} - {translations[x['Sector']]}\", axis=1)\n",
        "\n",
        "\n",
        "# Creating the bar plot with Seaborn\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Using a colorful palette\n",
        "barplot = sns.barplot(x='Label', y='Count', data=sector_counts, palette='tab10')\n",
        "\n",
        "# Adding the exact number of Articles on each bar\n",
        "for index, row in sector_counts.iterrows():\n",
        "    barplot.text(index, row['Count'], row['Count'], color='black', ha=\"center\")\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlabel('Sector (Arabic - English)')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.title('Distribution of Articles Among Sectors')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7vz1HnFaeBQ"
      },
      "source": [
        "## **Following light preprocessing as recommeneded by scholars, the Data Preprocessing Steps we need to apply for the integrated dataframe are the following:**\n",
        "\n",
        "1. Replacing User Mentions with relevant tokens ['مستخدم']\n",
        "2. Replacing URLS with relevant tokens ['رابط']\n",
        "3. Replacing Eamil Addresses with relevant tokens ['بريد']\n",
        "4. Removing HTML Tags\n",
        "5. Removing Emojis\n",
        "6. Normaliztaion (Stripping Arabic Tashkeel (Diacritics), Stripping Arabic Tatweel, Inserting White Spaces before and after all non Arabic digits or English digits or Arabic and English Alphabet or the 2 brackets, then inserts whitespace between words and numbers or numbers and words, Removing non Digit Repetition, Replacing Slash with Dash, mapping hindi numbers to Arabic ones)\n",
        "6. Handling Noisy Characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhY1Rqb8xoAB"
      },
      "source": [
        "#### **Selecting the needed columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ4IFwclxkUe"
      },
      "outputs": [],
      "source": [
        "merged_df_summary = merged_df[['Sector','Keyword', 'Title', 'Content']]\n",
        "merged_df_summary.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PYxgskxzZCM"
      },
      "source": [
        "#### **Merging the Title and the Content Columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oQ-1-mp0MH2"
      },
      "source": [
        "It was chosen to merge both the **Title** and **Content** Columns to enhance the model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbAiTuCzy4ZK"
      },
      "outputs": [],
      "source": [
        "merged_df_title_content = merged_df_summary.copy()\n",
        "merged_df_title_content['Title_Content'] = merged_df_summary['Title'] + \" \" + merged_df_summary['Content']\n",
        "merged_df_title_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwQYfKQUzkUK"
      },
      "outputs": [],
      "source": [
        "merged_df_title_content.drop(columns=['Title', 'Content'], inplace=True)\n",
        "merged_df_title_content.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvWx8Ku9Qpds"
      },
      "source": [
        "***Some Checks for Noisy Characters***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i40Pox81vlP"
      },
      "outputs": [],
      "source": [
        "# Investigating the English Words in the dataset\n",
        "\n",
        "def contains_english_words(text):\n",
        "\n",
        "    return bool(re.search(r'\\b[A-Za-z]+\\b', text))\n",
        "\n",
        "# Print Articles Content containing English words\n",
        "articles_with_english_words = merged_df_title_content[merged_df_title_content['Title_Content'].astype(str).apply(contains_english_words)]['Title_Content'].tolist()\n",
        "articles_with_english_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQBNFevvRvoC"
      },
      "source": [
        "**Comment:**\n",
        "Some Java Script Functions were found in the content which are considerd Noise. Also English and Arabic words need to be split by spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33r4AnE-bDV5"
      },
      "source": [
        "### **AraBERT Preprocessing Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2e_CTMJ2k-N"
      },
      "source": [
        "***AraBERT's preprocessing function is chosen for the follwoing reasons:***\n",
        "\n",
        "\n",
        "1. It is optimized for Arabic text, handling tasks such specific to the Arabic language as well as the newspaper articles, as normalization (including removing diacritics and tatweel), and special character handling (including HTML tages, URLs, emails, mentions). Leveraging these optimizations can help ensure that the input data is appropriately formatted for further processing by other Arabic BERT models.\n",
        "\n",
        "2. Arabic BERT models typically use similar tokenization schemes, especially that they are based on the same underlying architecture (BERT architecture). By using AraBERT's preprocessing function, the data is prepared in a tokenized format that is compatible with other Arabic BERT models, including ARBERT, facilitating seamless integration into the pretraining and fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xL-Vdfr8vKx"
      },
      "outputs": [],
      "source": [
        "# Importing AraBERT Preprocessor Function\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "# Initialize AraBERT Preprocessor\n",
        "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
        "arabert = ArabertPreprocessor(model_name=model_name, insert_white_spaces=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWmWmj40R5uq"
      },
      "source": [
        "***AraBERT Preprocess Function***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcybQuAs2lqd"
      },
      "source": [
        "Using **aubmindlab/bert-base-arabertv02** will ensure the following upon adopting the AraBERT Preprocessor function:\n",
        "\n",
        "\n",
        "*   keep_emojis(bool, optional, defaults to False): don't remove emojis while preprocessing --> will default to **False** for **AraBERTv02**\n",
        "\n",
        "*   remove_html_markup( bool, optional, defaults to True): Whether to remove html artfacts --> will default to **True** for **AraBERTv02**\n",
        "\n",
        "*   replace_urls_emails_mentions(bool, optional, defaults to True): Whether to replace email urls and mentions by special tokens --> will default to **True** for **AraBERTv02**\n",
        "\n",
        "*   strip_tashkeel(bool, optional, defaults to True): remove diacritics (FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SUKUN, SHADDA) --> will default to **True** for **AraBERTv02**\n",
        "\n",
        "*   strip_tatweel(bool, optional, defaults to True): remove tatweel '\\u0640' --> will default to **True** for **AraBERTv02**\n",
        "\n",
        "*   insert_white_spaces(bool, optional, defaults to True): insert whitespace before and after all non Arabic digits or English digits or Arabic and English Alphabet or the 2 brackets, then inserts whitespace between words and numbers or numbers and words --> will default to **True** for **AraBERTv02**\n",
        "\n",
        "*   remove_non_digit_repetition(bool, optional, defaults to True): replace repetition of more than 2 non-digit character with 2 of this character --> will default to **True** for **AraBERTv02**\n",
        "\n",
        "*   replace_slash_with_dash(bool, optional, defaults to None): --> will be automatically set to **True** for **AraBERTv02**\n",
        "\n",
        "*   map_hindi_numbers_to_arabic(bool, optional, defaults to None): Replaces hindi numbers with the corresponding Arabic one. ex: \"١٩٩٥\" --> \"1995\" --> will be automatically set to **True** for **AraBERTv02**\n",
        "\n",
        "*   apply_farasa_segmentation(bool, optional, defaults to None): --> will be automatically set to **None** for **AraBERTv02**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD_5eUdy9vFf"
      },
      "outputs": [],
      "source": [
        "# AraBERT preprocess function\n",
        "def preprocess_text(text):\n",
        "    return arabert.preprocess(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMqtLYXeSi-W"
      },
      "outputs": [],
      "source": [
        "# Apply it on the dataset and generate a new column called Preprocessed_Content\n",
        "\n",
        "merged_df_title_content.loc[:,'Processed_Content'] = merged_df_title_content.loc[:,'Title_Content'].apply(preprocess_text)\n",
        "merged_df_title_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMo3D5hvTDdd"
      },
      "source": [
        "***Checking English Words again after applying AraBERT Preprocessor Function***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztWqnuv0S0iL"
      },
      "outputs": [],
      "source": [
        "# Print Articles Content containing English words\n",
        "articles_with_english_words = merged_df_title_content[merged_df_title_content['Processed_Content'].astype(str).apply(contains_english_words)]['Processed_Content'].tolist()\n",
        "articles_with_english_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgWFx7DKTYDO"
      },
      "source": [
        "**Comment:**\n",
        "\n",
        "- Not all English words are correctly seperated from the Arabic ones\n",
        "- Emails are not correctly replaced by [بريد] token since they are represented in a protected format\n",
        "- One Java Script Function is still not handled (mbInitialization ( ) ; mbCallAd ( 99577 , 663572 , 40222 ) ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haTgNtuhtocl"
      },
      "source": [
        "***Replacing emails with \"[بريد]\" since they have a special format in the dataset which cannot be recognized bt AraBERT***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSeCwQFttntM"
      },
      "outputs": [],
      "source": [
        "def replace_special_tokens(text):\n",
        "\n",
        "\n",
        "    return text.replace(\"[email protected]\" ,'[بريد]')\n",
        "\n",
        "merged_df_title_content['Processed_Content'] = merged_df_title_content['Processed_Content'].astype(str).apply(replace_special_tokens)\n",
        "merged_df_title_content['Processed_Content'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6lUC_DXC3na"
      },
      "source": [
        "***English words were not correctly seperated from the Arabic ones. It was decided to separate English and Arabic words that are not correctly separated by spaces.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUZb2IrW_gZy"
      },
      "outputs": [],
      "source": [
        "def separate_english_arabic(text):\n",
        "\n",
        "    # Add space between Arabic and English words\n",
        "    text = re.sub(r'([a-zA-Z])([\\u0621-\\u064A\\u0660-\\u0669])', r'\\1 \\2', text)  # Add space between English and Arabic characters\n",
        "    text = re.sub(r'([\\u0621-\\u064A\\u0660-\\u0669])([a-zA-Z])', r'\\1 \\2', text)  # Add space between Arabic and English characters\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "merged_df_title_content['Processed_Content']= merged_df_title_content['Processed_Content'].astype(str).apply(separate_english_arabic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOuOQ3EmSps8"
      },
      "source": [
        "***Removing the Javascript codes represnting noise***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhrHl6eVK98I"
      },
      "outputs": [],
      "source": [
        "\n",
        "def contains_javascript_function(text):\n",
        "\n",
        "\n",
        "\n",
        "    if 'mbInitialization' in text:\n",
        "\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def remove_noisy_characters(text):\n",
        "\n",
        "    return re.sub(r'mbInitialization\\s*\\(\\s*\\)\\s*;\\s*mbCallAd\\s*\\(\\s*99577\\s*,\\s*663572\\s*,\\s*40222\\s*\\)\\s*;', '', text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Iedr537VX6J"
      },
      "outputs": [],
      "source": [
        "# Before removing the Java Script Function\n",
        "\n",
        "reviews = merged_df_title_content[merged_df_title_content['Processed_Content'].astype(str).apply(contains_javascript_function)]['Processed_Content'].tolist()\n",
        "\n",
        "reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXWLhsB48CcF"
      },
      "outputs": [],
      "source": [
        "# Before removing the Java Script Function\n",
        "merged_df_title_content['Processed_Content'] = merged_df_title_content['Processed_Content'].astype(str).apply(remove_noisy_characters)\n",
        "\n",
        "\n",
        "\n",
        "reviews = merged_df_title_content[merged_df_title_content['Processed_Content'].astype(str).apply(contains_javascript_function)]['Processed_Content'].tolist()\n",
        "\n",
        "reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIOchWrdgLlL"
      },
      "outputs": [],
      "source": [
        "merged_df_title_content['Processed_Content'] = merged_df_title_content['Processed_Content'].astype(str).apply(remove_noisy_characters)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO3-IODxUelj"
      },
      "outputs": [],
      "source": [
        "# After Removing the Java Script Function\n",
        "reviews = merged_df_title_content[merged_df_title_content['Processed_Content'].astype(str).apply(contains_javascript_function)]['Processed_Content'].tolist()\n",
        "\n",
        "reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP4trZCGV7oW"
      },
      "source": [
        "# **Saving the Preprocessed Dataset to Google Drive for further usage in the DAPT approach**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjXiUdKsWYY-"
      },
      "outputs": [],
      "source": [
        "# dropping the Title_Content Column (the one that is not preprocessed)\n",
        "\n",
        "merged_df_title_content.drop(columns=['Title_Content'], inplace=True)\n",
        "merged_df_title_content.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEdpgaCDrhex"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "merged_df_title_content.to_csv('/content/preprocessed_dataset.csv', index=False)\n",
        "\n",
        "\n",
        "# Destination file path on Drive\n",
        "drive_destination_file = \"/content/drive/My Drive/######\"  # Update this as needed\n",
        "\n",
        "# Copy the file from the source path to the destination path\n",
        "shutil.copy('/content/preprocessed_dataset.csv', drive_destination_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
