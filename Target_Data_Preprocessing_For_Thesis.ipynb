{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2MQVWW7ZE8V"
      },
      "source": [
        "# **Installing the Required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1OZ1saOZkVC"
      },
      "outputs": [],
      "source": [
        "!pip install arabert\n",
        "!pip install nltk\n",
        "!pip install arabic_reshaper\n",
        "!pip install python-bidi\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b94R35Z57576"
      },
      "source": [
        "# **Importing the required Libraries and Tools**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LCsbzQKZl8a"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainerCallback, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFSczQQI8BQO"
      },
      "source": [
        "# **Constructing a Dataframe from the Target data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14uuUG1ZEwqN"
      },
      "source": [
        "The Target Data about Citizen Reviews is provided in **.txt** format, where each review is available in one seperate **.txt** file and every category of reviews relevant to a particular government sector is stored in corresponding folder. So, we have 10 Folders in total represnting the 10 government sectors.\n",
        "\n",
        "The following is required to be able to use it in this project:\n",
        "\n",
        "1. Extract and Parse the text content from the **.txt** files within each folder.\n",
        "2. Create and save a distinct **.csv** file for each folder (government sector) for easier future management.\n",
        "3. Create a Merged Dataframe from the Sector **.csv** files and Optionally save it as one integrated **.csv** on Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3esGMxHerK"
      },
      "source": [
        "***Mounting Google Drive and defining the Data Folder Path***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncWmdcWDLcw-"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hTGFc2zfnnk"
      },
      "outputs": [],
      "source": [
        "# Defining the path to the main folder and the output folder\n",
        "\n",
        "# Folder Main Path with the TXT Files\n",
        "drive_main_folder_path = '/content/drive/My Drive/######'\n",
        "\n",
        "# Folder Path for CSV Files for each Sector\n",
        "drive_output_folder_path = '/content/drive/My Drive/######'\n",
        "\n",
        "# Folder Path for one CSV File for all the Reviews (Merging All Secor CSV Files)\n",
        "merged_reviews_drive_output_folder_path = '/content/drive/My Drive/######'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx7ywXU-tMvl"
      },
      "outputs": [],
      "source": [
        "# Function to process each category and save as a CSV file\n",
        "def process_and_save_category(category_path, category_name, output_folder_path):\n",
        "    data = {'Sector': [], 'Content': []}\n",
        "\n",
        "    for file_name in os.listdir(category_path):\n",
        "        if file_name.endswith('.txt'):\n",
        "            file_path = os.path.join(category_path, file_name)\n",
        "            with open(file_path, 'r') as file:\n",
        "                content = file.read()\n",
        "                data['Sector'].append(category_name)\n",
        "                data['Content'].append(content)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    output_file_path = os.path.join(output_folder_path, f\"{category_name}.csv\")\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKtduOwnf8Gf"
      },
      "outputs": [],
      "source": [
        "# Processing each category folder and saving a CSV file for each Government Sector in the specified folder\n",
        "all_dataframes = []\n",
        "\n",
        "for category in os.listdir(drive_main_folder_path):\n",
        "    category_path = os.path.join(drive_main_folder_path, category)\n",
        "    if os.path.isdir(category_path):\n",
        "        df = process_and_save_category(category_path, category, drive_output_folder_path)\n",
        "        all_dataframes.append(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLz_OR23gWdR"
      },
      "outputs": [],
      "source": [
        "# Saving the merged DataFrame to a CSV file and Storing it in Google Drive\n",
        "\n",
        "\n",
        "merged_reviews_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "merged_output_file_path = os.path.join(merged_reviews_drive_output_folder_path, '#######.csv') # Replace by actual dataset path\n",
        "merged_reviews_df.to_csv(merged_output_file_path, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3nJzrPCFMGL"
      },
      "source": [
        "# **Data Understanding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj0uy7a9ZmJP"
      },
      "source": [
        "# **Data Inspection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVshkwL1BA5t"
      },
      "source": [
        "***Inspecting the Merged Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4Drl-rWgXLJ"
      },
      "outputs": [],
      "source": [
        "df.duplicated(subset=['Content']).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqMHDut9YUVO"
      },
      "outputs": [],
      "source": [
        "# To set the display width for pandas DataFrame\n",
        "pd.set_option('display.max_colwidth', 80)  # You can increase the number to widen the content\n",
        "\n",
        "# getting number of rows and columns for the dataset\n",
        "print(f\"Number of Rows and Columns in Target Reviews Dataset: {merged_reviews_df.shape}\\n\")\n",
        "\n",
        "# shuffle the dataset\n",
        "merged_reviews_df = merged_reviews_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Displaying the merged DataFrame\n",
        "\n",
        "merged_reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pJyG1gJSLbc"
      },
      "outputs": [],
      "source": [
        "# getting number of rows and columns for the dataset\n",
        "\n",
        "print(f\"Number of Rows and Columns in Target Reviews Dataset: {merged_reviews_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44uqO2G49U34"
      },
      "outputs": [],
      "source": [
        "# Manually define the Arabic-to-English translation dictionary\n",
        "translations = {\n",
        "    'الاتصالات': 'Communication',\n",
        "    'البنوك': 'Banking',\n",
        "    'البيئة': 'Environment',\n",
        "    'التعليم': 'Education',\n",
        "    'التموين': 'Supply',\n",
        "    'الزراعة': 'Agriculture',\n",
        "    'الصحة': 'Healthcare',\n",
        "    'القضاء': 'Judiciary',\n",
        "    'الكهرباء': 'Electricity',\n",
        "    'المياه والصرف الصحي': 'Water and Sanitation'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAV6TfBP_gqS"
      },
      "outputs": [],
      "source": [
        "# getting unique number of Sectors and their names\n",
        "\n",
        "print(\"No. of Unique Sectors: \" + str(merged_reviews_df['Sector'].nunique()))\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(\"Unique Sectors: \\n\")\n",
        "print(merged_reviews_df.Sector.unique())\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(\"English Traslation: \\n\")\n",
        "list(translations.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQKzTD6Suww4"
      },
      "outputs": [],
      "source": [
        "# getting the info of dataframes\n",
        "\n",
        "print(merged_reviews_df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpLQ5mlcDaSM"
      },
      "source": [
        "**Comment:**\n",
        "It is obvious that there is no Null values in any of the dataframes and the datset is 32.0+ KB which is a reasonable size for finetuning the ARBERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCV0YQIMbcaU"
      },
      "outputs": [],
      "source": [
        "# get rows with duplicates\n",
        "\n",
        "merged_reviews_df[merged_reviews_df.duplicated(keep=False)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKmDL3qPD-s8"
      },
      "outputs": [],
      "source": [
        "# Getting a detailed Summary about the dataframe including the Categorical ones\n",
        "\n",
        "merged_reviews_df.describe(include='all')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G4ByRTOUN30"
      },
      "outputs": [],
      "source": [
        "# Checking duplictates and null values\n",
        "print('Null Values: ', merged_reviews_df.isnull().sum(axis=0).sum(),\n",
        "      '\\n',\n",
        "      'Duplicate Values: ', merged_reviews_df.duplicated(subset='Content').sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCmuPgnN1Ccm"
      },
      "outputs": [],
      "source": [
        "#getting duplicate content\n",
        "\n",
        "merged_reviews_df[merged_reviews_df.duplicated(subset=['Content'], keep=False)].sort_values(by='Content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A97UqVjT2I2C"
      },
      "source": [
        "**Comment:**\n",
        "No Duplicates or Null Values found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67q-d9GxF7lq"
      },
      "source": [
        "***Barplot showing the Reviews Distribution over Sectors***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPBftKfAmicN"
      },
      "outputs": [],
      "source": [
        "# Dictionary for Arabic to English translations\n",
        "\n",
        "# Count the number of rows per sector\n",
        "sector_counts = merged_reviews_df['Sector'].value_counts().reset_index()\n",
        "sector_counts.columns = ['Sector', 'Count']\n",
        "\n",
        "# Reverse the letters of each sector name for readability\n",
        "sector_counts['Reversed_Sector'] = sector_counts['Sector'].apply(lambda x: get_display(arabic_reshaper.reshape(x)))\n",
        "sector_counts['Label'] = sector_counts.apply(lambda x: f\"{x['Reversed_Sector']} - {translations[x['Sector']]}\", axis=1)\n",
        "\n",
        "\n",
        "# Create the bar plot with Seaborn\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Use a colorful palette\n",
        "barplot = sns.barplot(x='Label', y='Count', data=sector_counts, palette='tab10')\n",
        "\n",
        "# Annotate the bars with larger font size\n",
        "for b in barplot.patches:\n",
        "    barplot.annotate(format(b.get_height(), '.0f'),\n",
        "               (b.get_x() + b.get_width() / 2., b.get_height()),\n",
        "                ha='center', va='baseline', fontsize=12, color='black')  # Adjust fontsize here\n",
        "\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlabel('Sector (Arabic - English)', fontsize=18)\n",
        "plt.ylabel('Number of Reviews', fontsize=18)\n",
        "plt.title('Distribution of Reviews Among Sectors', fontsize=20, fontweight='bold')\n",
        "\n",
        "# set bar value font size\n",
        "\n",
        "plt.xticks(fontsize=14, rotation=45, ha='right')\n",
        "plt.yticks(fontsize=14)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG02MnVFaC95"
      },
      "source": [
        "# **Data Preparation and Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g_b8mx7K0uk"
      },
      "source": [
        "## **Following amost the same light preprocessing applied in Further pretraining as recommeneded by scholars, we just added some important ones based on this dataset characteristics**\n",
        "\n",
        "The Data Preprocessing Steps we need to apply for the integrated dataframe are the following:\n",
        "\n",
        "1. Adjust Sentence and Paragraph spaces and line breaks.\n",
        "2. Use AraBERT preprocess function to handle most of the preprocessing steps\n",
        "3. Replacing ministry names with the general string Token. This normalization enabled the model to focus on the relevant context to classify reviews rather than relying on the explicit mention of the ministry name.\n",
        "4. Use Specific Tokens for Relevant Companies: Replace company names with sector-specific tokens if those companies are closely tied to specific sectors.\n",
        "5. Normalize the punctuations (convert Engligh to Arabic ones)\n",
        "6. Selective Punctuation Removal: retain the following set {%-؟!.:؛،[]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekzdDKMaUJOl"
      },
      "source": [
        "***Adjusting Sentence spaces and Paragraph line breaks.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Ailb-6UN-a"
      },
      "outputs": [],
      "source": [
        "def adjust_spaces_line_breaks(text):\n",
        "\n",
        "    # Remove leading and trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    # Replace multiple line breaks (indicating paragraphs) with a placeholder\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
        "\n",
        "    # Replace remaining single line breaks with a space\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3keK3w2V8IQ"
      },
      "outputs": [],
      "source": [
        "# before applying the adjust_spaces_line_breaks function\n",
        "merged_reviews_df['Content'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GT92eUrVOnM"
      },
      "outputs": [],
      "source": [
        "# applying the adjust_spaces_line_breaks function to the DataFrame\n",
        "\n",
        "merged_reviews_df['Processed_Content'] = merged_reviews_df['Content'].apply(adjust_spaces_line_breaks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9AhXrOPRwKx"
      },
      "outputs": [],
      "source": [
        "merged_reviews_df['Processed_Content'] = merged_reviews_df['Content'].apply(adjust_spaces_line_breaks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4z_ENOYWGsj"
      },
      "outputs": [],
      "source": [
        "# after applying the adjust_spaces_line_breaks function\n",
        "\n",
        "merged_reviews_df['Processed_Content'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY6m86ITUR_h"
      },
      "source": [
        "***AraBERT Preprocess Function***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWtUyrpEUWRb"
      },
      "outputs": [],
      "source": [
        "# Initialize AraBERT Preprocessor\n",
        "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "\n",
        "def arabert_preprocess(text):\n",
        "    return arabert_prep.preprocess(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHfDiGo9Wo7_"
      },
      "outputs": [],
      "source": [
        "# after applying the arabert_preprocess function\n",
        "\n",
        "merged_reviews_df['Processed_Content'] = merged_reviews_df['Processed_Content'].apply(arabert_preprocess)\n",
        "merged_reviews_df['Processed_Content'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0QjwYNWuiAz"
      },
      "source": [
        "***Use Specific Tokens for Relevant Companies: Replace company names with sector-specific tokens if those companies are closely tied to specific sectors.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl_K24RlbD_m"
      },
      "outputs": [],
      "source": [
        "\n",
        "def replace_company_tokens(text, token_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, token_dict.keys())) + r')\\b')\n",
        "    return pattern.sub(lambda match: token_dict[match.group(0)], text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keQjF3bQExHT"
      },
      "outputs": [],
      "source": [
        "#  Replace any whole word occurrence of dictionary keys in the text with their corresponding values.\n",
        "\n",
        "company_tokens = {\n",
        "'مستشفى دراو المركزي': '[مؤسسة_أ]',\n",
        "'مستشفى القباري': '[مؤسسة_ب]',\n",
        "'مستشفى سعاد كفافي': '[مؤسسة_ج]',\n",
        "'مستشفى منشية البكري': '[مؤسسة_د]',\n",
        "'مستشفى العدوة المركزي': '[مؤسسة_ه]',\n",
        "'مستشفى العياط': '[مؤسسة_ز]',\n",
        "'مستشفى المنيرة': '[مؤسسة_ح]',\n",
        "'مستشفى الفكرية المركزي': '[مؤسسة_م]',\n",
        "'مستشفى القباري': '[مؤسسة_س]',\n",
        "'مستشفى النيل': '[مؤسسة_ش]',\n",
        "'مستشفى المبرة': '[مؤسسة_ف]',\n",
        "'مستشفى المطرية': '[مؤسسة_ت]',\n",
        "'مستشفى الحسين الجامعي': '[مؤسسة_خ]',\n",
        "'مستشفى ٦ أكتوبر بالدقي': '[مؤسسة_ث]',\n",
        "'معهد السكر': '[مؤسسة_ك]',\n",
        "'مستشفى أطفال مصر': '[مؤسسة_ؤ]',\n",
        "'مستشفى مدينة نصر': '[مؤسسة_ظ]',\n",
        "'مستشفى الرمد الجديدة بكفر الشيخ': '[مؤسسة_غ]',\n",
        "'مستشفى ٦ أكتوبر': '[مؤسسة_ى]',\n",
        "'مستشفى شرق المدينه بالإسكندرية': '[مؤسسة_و]',\n",
        "'مستشفى العديسات': '[مؤسسة_ع]',\n",
        "\n",
        "'جامعة القاهرة':  '[مؤسسة_ق]',\n",
        "\n",
        "'لمنظومة الشكاوى الموحد':'[مؤسسة_ن]',\n",
        "\n",
        "'النجاح وأحمد عرابي ) بمركز بدر' : '[مؤسسة_ل]',\n",
        "'مدرسة أبوحزام الإبتدائية التابعة لإدارة نجع حمادى التعليمية'  : '[مؤسسة_ة]',\n",
        "'بشركة أبوقير': '[مؤسسة_ر]',\n",
        "\n",
        "'جمعية الغريزات الزراعية بالغريزات مركز المراغة': '[مؤسسة_ذ]',\n",
        "'جمعية الهيطة الزراعية التابعة لمركز صان الحجر' : '[مؤسسة_لا]',\n",
        "'جمعية أبو طه الزراعية مركز بلقاس' : '[مؤسسة_أي]',\n",
        "\n",
        "'البنك الأهلي': '[مؤسسة_عغ]',\n",
        "'بنك الإمارات دبي الوطني' : '[مؤسسة_يس]',\n",
        "'البنك التجاري الدولي' : '[مؤسسة_را]',\n",
        "'بنك ناصر الاجتماعي' : '[مؤسسة_ون]',\n",
        "'بنك ناصر' : '[مؤسسة_رض]',\n",
        "'بنك وفا التجاري' : '[مؤسسة_دج]',\n",
        "'بنك الإسكندرية' : '[مؤسسة_بك]',\n",
        "'بنك الأهلي قطر' : '[مؤسسة_سش]',\n",
        "'بالوحدة المحلية بفيشا الكبري وتتبع بنك مصر فرع منوف' : '[مؤسسة_حز]',\n",
        "'البنك التجاري الدولي': '[مؤسسة_ضت]',\n",
        "'CIB' : '[مؤسسة_قر]',\n",
        "'cib' : '[مؤسسة_زو]',\n",
        "'البنك الإمارات الوطني': '[مؤسسة_هك]',\n",
        "'HSBC': '[مؤسسة_فش]',\n",
        "'Abu Dhabi Commercial Bank': '[مؤسسة_هخ]',\n",
        "'البنك المركزي': '[مؤسسة_يك]',\n",
        "\n",
        "'وي': '[مؤسسة_غى]',\n",
        "'أورانج': '[مؤسسة_عة]',\n",
        "'فودافون': '[مؤسسة_سط]',\n",
        "'الشركة المصرية للاتصالات': '[مؤسسة_ظب]'\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z6LnwmgtiX7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# apply the replace company_tokens function on the data\n",
        "merged_reviews_df['Processed_Content'] = merged_reviews_df['Processed_Content'].apply(lambda x: replace_company_tokens(x, company_tokens))\n",
        "merged_reviews_df['Processed_Content'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pi7Epm6SrFi"
      },
      "outputs": [],
      "source": [
        "def replace_sector_names(text, token_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, token_dict.keys())) + r')\\b')\n",
        "    return pattern.sub(lambda match: token_dict[match.group(0)], text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz_DCPlPE1_y"
      },
      "outputs": [],
      "source": [
        "sector_names = {\n",
        "'وزارة الصحة': '[الوزارة]',\n",
        "'وزيرة الصحة': '[الوزيرة]',\n",
        "'وزير الصحة': '[الوزير]',\n",
        "\n",
        "\n",
        "'وزارة الزراعة واستصلاح الأراضي':'[الوزارة]',\n",
        "'وزيرة الزراعة واستصلاح الأراضي':'[الوزيرة]',\n",
        "'وزير الزراعة واستصلاح الأراضي':'[الوزير]',\n",
        "\n",
        "\n",
        "'وزارة الزراعة': '[الوزارة]',\n",
        "'وزيرة الزراعة': '[الوزيرة]',\n",
        "'وزير الزراعة': '[الوزير]',\n",
        "\n",
        "'وزارة الري': '[الوزارة]',\n",
        "'وزيرة الري': '[الوزيرة]',\n",
        "'وزير الري': '[الوزير]',\n",
        "\n",
        "'وزارة الموراد المائية والري': '[الوزارة]',\n",
        "'وزيرة الموراد المائية والري': '[الوزيرة]',\n",
        "'وزير الموراد المائية والري': '[الوزير]',\n",
        "\n",
        "'وزارة التربية والتعليم': '[الوزارة]',\n",
        "'وزيرة التربية والتعليم': '[الوزيرة]',\n",
        "'وزير التربية والتعليم': '[الوزير]',\n",
        "\n",
        "'وزارة التعليم العالي': '[الوزارة]',\n",
        "'وزيرة التعليم العالي': '[الوزيرة]',\n",
        "'وزير التعليم العالي': '[الوزير]',\n",
        "\n",
        "'وزارة المياه والصرف الصحي': '[الوزارة]',\n",
        "'وزيرة المياه والصرف الصحي': '[الوزيرة]',\n",
        "'وزير المياه والصرف الصحي': '[الوزير]',\n",
        "\n",
        "'وزارة الكهرباء': '[الوزارة]',\n",
        "'وزيرة الكهرباء': '[الوزيرة]',\n",
        "'وزير الكهرباء': '[الوزير]',\n",
        "\n",
        "'وزارة العدل': '[الوزارة]',\n",
        "'وزيرة العدل': '[الوزيرة]',\n",
        "'وزير العدل': '[الوزير]',\n",
        "\n",
        "'وزارة التموين': '[الوزارة]',\n",
        "'وزيرة التموين': '[الوزيرة]',\n",
        "'وزير التموين': '[الوزير]',\n",
        "\n",
        "'وزارة البيئة': '[الوزارة]',\n",
        "'وزيرة البيئة': '[الوزيرة]',\n",
        "'وزير البيئة': '[الوزير]',\n",
        "\n",
        "\n",
        "'وزارة الاتصالات وتكنولوجيا المعلومات': '[الوزارة]',\n",
        "'وزيرة الاتصالات وتكنولوجيا المعلومات': '[الوزيرة]',\n",
        "'وزير الاتصالات وتكنولوجيا المعلومات': '[الوزير]',\n",
        "\n",
        "'وزارة الاتصالات': '[الوزارة]',\n",
        "'وزيرة الاتصالات': '[الوزيرة]',\n",
        "'وزير الاتصالات': '[الوزير]',\n",
        "\n",
        "\n",
        "'لوزارة الصحة': '[الوزارة] ل',\n",
        "'لوزيرة الصحة': '[الوزيرة] ل',\n",
        "'لوزير الصحة': '[الوزير] ل',\n",
        "\n",
        "\n",
        "'لوزارة الزراعة واستصلاح الأراضي':'[الوزارة] ل',\n",
        "'لوزيرة الزراعة واستصلاح الأراضي':'[الوزيرة] ل',\n",
        "'لوزير الزراعة واستصلاح الأراضي':'[الوزير] ل',\n",
        "\n",
        "'لوزارة الزراعة': '[الوزارة] ل',\n",
        "'لوزيرة الزراعة': '[الوزيرة] ل',\n",
        "'لوزير الزراعة': '[الوزير] ل',\n",
        "\n",
        "\n",
        "'لوزارة الري': '[الوزارة] ل',\n",
        "'لوزيرة الري': '[الوزيرة] ل',\n",
        "'لوزير الري': '[الوزير] ل',\n",
        "\n",
        "'لوزارة المواد المائية والري': '[الوزارة] ل',\n",
        "'لوزيرة المواد المائية والري': '[الوزيرة] ل',\n",
        "'لوزير المواد المائية والري': '[الوزير] ل',\n",
        "\n",
        "'لوزارة التربية والتعليم': '[الوزارة] ل',\n",
        "'لوزيرة التربية والتعليم': '[الوزيرة] ل',\n",
        "'لوزير التربية والتعليم': '[الوزير] ل',\n",
        "\n",
        "'لوزارة التعليم العالي': '[الوزارة] ل',\n",
        "'لوزيرة التعليم العالي': '[الوزيرة] ل',\n",
        "'لوزير التعليم العالي': '[الوزير] ل',\n",
        "\n",
        "'لوزارة المياه والصرف الصحي': '[الوزارة] ل',\n",
        "'لوزيرة المياه والصرف الصحي': '[الوزيرة] ل',\n",
        "'لوزير المياه والصرف الصحي': '[الوزير] ل',\n",
        "\n",
        "'لوزارة الكهرباء': '[الوزارة] ل',\n",
        "'لوزيرة الكهرباء': '[الوزيرة] ل',\n",
        "'لوزير الكهرباء': '[الوزير] ل',\n",
        "\n",
        "'لوزارة العدل': '[الوزارة] ل',\n",
        "'لوزيرة العدل': '[الوزيرة] ل',\n",
        "'لوزير العدل': '[الوزير] ل',\n",
        "\n",
        "'لوزارة التموين': '[الوزارة] ل',\n",
        "'لوزيرة التموين': '[الوزيرة] ل',\n",
        "'لوزير التموين': '[الوزير] ل',\n",
        "\n",
        "'لوزارة البيئة': '[الوزارة] ل',\n",
        "'لوزيرة البيئة': '[الوزيرة] ل',\n",
        "'لوزير البيئة': '[الوزير] ل',\n",
        "\n",
        "\n",
        "'لوزارة الاتصالات وتكنولوجيا المعلومات': '[الوزارة] ل',\n",
        "'لوزيرة الاتصالات وتكنولوجيا المعلومات': '[الوزيرة] ل',\n",
        "'لوزير الاتصالات وتكنولوجيا المعلومات': '[الوزير] ل',\n",
        "\n",
        "'لوزارة الاتصالات': '[الوزارة] ل',\n",
        "'لوزيرة الاتصالات': '[الوزيرة] ل',\n",
        "'لوزير الاتصالات': '[الوزير] ل',\n",
        "\n",
        "\n",
        "\n",
        "'بوزارة الصحة': '[الوزارة] ب',\n",
        "'بوزيرة الصحة': '[الوزيرة] ب',\n",
        "'بوزير الصحة': '[الوزير] ب',\n",
        "\n",
        "\n",
        "'بوزارة الزراعة واستصلاح الأراضي':'[الوزارة] ب',\n",
        "'بوزيرة الزراعة واستصلاح الأراضي':'[الوزيرة] ب',\n",
        "'بوزير الزراعة واستصلاح الأراضي':'[الوزير] ب',\n",
        "\n",
        "'بوزارة الزراعة': '[الوزارة] ب',\n",
        "'بوزيرة الزراعة': '[الوزيرة] ب',\n",
        "'بوزير الزراعة': '[الوزير] ب',\n",
        "\n",
        "\n",
        "'بوزارة الري': '[الوزارة] ب',\n",
        "'بوزيرة الري': '[الوزيرة] ب',\n",
        "'بوزير الري': '[الوزير] ب',\n",
        "\n",
        "'بوزارة المواد المائية والري': '[الوزارة] ب',\n",
        "'بوزيرة المواد المائية والري': '[الوزيرة] ب',\n",
        "'بوزير المواد المائية والري': '[الوزير] ب',\n",
        "\n",
        "'بوزارة التربية والتعليم': '[الوزارة] ب',\n",
        "'بوزيرة التربية والتعليم': '[الوزيرة] ب',\n",
        "'بوزير التربية والتعليم': '[الوزير] ب',\n",
        "\n",
        "'بوزارة التعليم العالي': '[الوزارة] ب',\n",
        "'بوزيرة التعليم العالي': '[الوزيرة] ب',\n",
        "'بوزير التعليم العالي': '[الوزير] ب',\n",
        "\n",
        "'بوزارة المياه والصرف الصحي': '[الوزارة] ب',\n",
        "'بوزيرة المياه والصرف الصحي': '[الوزيرة] ب',\n",
        "'بوزير المياه والصرف الصحي': '[الوزير] ب',\n",
        "\n",
        "'بوزارة الكهرباء': '[الوزارة] ب',\n",
        "'بوزيرة الكهرباء': '[الوزيرة] ب',\n",
        "'بوزير الكهرباء': '[الوزير] ب',\n",
        "\n",
        "'بوزارة العدل': '[الوزارة] ب',\n",
        "'بوزيرة العدل': '[الوزيرة] ب',\n",
        "'بوزير العدل': '[الوزير] ب',\n",
        "\n",
        "'بوزارة التموين': '[الوزارة] ب',\n",
        "'بوزيرة التموين': '[الوزيرة] ب',\n",
        "'بوزير التموين': '[الوزير] ب',\n",
        "\n",
        "'بوزارة البيئة': '[الوزارة] ب',\n",
        "'بوزيرة البيئة': '[الوزيرة] ب',\n",
        "'بوزير البيئة': '[الوزير] ب',\n",
        "\n",
        "\n",
        "'بوزارة الاتصالات وتكنولوجيا المعلومات': '[الوزارة] ب',\n",
        "'بوزيرة الاتصالات وتكنولوجيا المعلومات': '[الوزيرة] ب',\n",
        "'بوزير الاتصالات وتكنولوجيا المعلومات': '[الوزير] ب',\n",
        "\n",
        "'بوزارة الاتصالات': '[الوزارة] ب',\n",
        "'بوزيرة الاتصالات': '[الوزيرة] ب',\n",
        "'بوزير الاتصالات': '[الوزير] ب'\n",
        "\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMRDUa9uSrFj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# applying the replace_location_names function\n",
        "merged_reviews_df['Processed_Content'] = merged_reviews_df['Processed_Content'].apply(lambda x: replace_sector_names(x, sector_names))\n",
        "merged_reviews_df['Processed_Content'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hC0AftN6A87"
      },
      "source": [
        "***Handling Punctuations***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gc-rnXMyDhw"
      },
      "outputs": [],
      "source": [
        "# Normalizing Punctuations (changing English to Arabic ones)\n",
        "def normalize_punctuations(text):\n",
        "\n",
        "\n",
        "  translation_table = str.maketrans({'.': '.', ',': '،', '?': '؟', '!': '!', ';': '؛', ':': '؛'})\n",
        "  return text.translate(translation_table)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPI2FuWO7e1n"
      },
      "outputs": [],
      "source": [
        "# applying the normalize_punctuations function on merged_reviews_df\n",
        "\n",
        "merged_reviews_df['Processed_Content'] = merged_reviews_df['Processed_Content'].apply(normalize_punctuations)\n",
        "merged_reviews_df['Processed_Content'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MjLiBvs6L9N"
      },
      "source": [
        "***Applying selective punctuation removal***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ejbzj6RBpXX"
      },
      "outputs": [],
      "source": [
        "# Defining selective_punctuation_removal function\n",
        "def selective_punctuation_removal(text):\n",
        "    allowed_punctuations = {'%', '٪','-', '؟', '!', '.', ':', '؛', '،', '[', ']'}\n",
        "    pattern = re.compile(r'[^%\\-\\؟\\!\\.\\:\\؛\\،\\[\\]\\w\\s0-9]')\n",
        "    return pattern.sub('', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qxA7C_kCXEz"
      },
      "outputs": [],
      "source": [
        "# Applying selective_punctuation_removal function on merged_reviews_df\n",
        "\n",
        "merged_reviews_df['Processed_Content'] = merged_reviews_df['Processed_Content'].apply(selective_punctuation_removal)\n",
        "merged_reviews_df['Processed_Content'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvWx8Ku9Qpds"
      },
      "source": [
        "***Some Checks for Noisy Characters***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i40Pox81vlP"
      },
      "outputs": [],
      "source": [
        "# Investigating the English Words in the dataset\n",
        "\n",
        "def contains_english_words(text):\n",
        "\n",
        "    return bool(re.search(r'\\b[A-Za-z]+\\b', text))\n",
        "\n",
        "# Print Reviews Content containing English words\n",
        "reviews_with_english_words = merged_reviews_df[merged_reviews_df['Processed_Content'].astype(str).apply(contains_english_words)]['Processed_Content'].tolist()\n",
        "reviews_with_english_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjXiUdKsWYY-"
      },
      "outputs": [],
      "source": [
        "# dropping the Content Column (the one that is not preprocessed)\n",
        "\n",
        "merged_reviews_df.drop(columns=['Content'], inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP4trZCGV7oW"
      },
      "source": [
        "# **Saving the Train and Test Data Splits to Google Drive for further usage in the Training and Evaluating SVM as well as Finetuning and Evaluating Both ARBERT Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i89r4Zu_2qE6"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "merged_reviews_df.to_csv('/content/#####', index=False)\n",
        "\n",
        "\n",
        "# Destination file path on Drive\n",
        "drive_destination_file = \"/content/drive/My Drive/#####\"\n",
        "\n",
        "# Copy the file from the source path to the destination path\n",
        "shutil.copy('/content/#####', drive_destination_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeA5b8VfoFVQ"
      },
      "source": [
        "# **Splitting the Target Data for the Normal Scenario**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnocnHlB3KIS"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "drive_destination_file = \"/content/drive/My Drive/#####\"\n",
        "\n",
        "# Reading the CSV file into a DataFrame\n",
        "preprocessed_df = pd.read_csv(drive_destination_file)\n",
        "\n",
        "# Displaying the DataFrame\n",
        "preprocessed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCEt2tEYU0d2"
      },
      "outputs": [],
      "source": [
        "# Using the below function to split the data into train+val and test sets (80% train+val, 20% test) for the first scenario: Standard Splitting\n",
        "\n",
        "def data_split(df, test_size, stratify_coulmn): # stratify_coulmn here is the Sector column\n",
        "    train_val_data, test_data = train_test_split(df, test_size=test_size, stratify=stratify_coulmn, random_state=1)\n",
        "    return train_val_data, test_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3TJrQXdekUx"
      },
      "outputs": [],
      "source": [
        "# Priniting the function output to check the sizes of the train and test splits\n",
        "\n",
        "# Example usage:\n",
        "train_data, test_data = data_split(preprocessed_df, 0.2, preprocessed_df['Sector'])\n",
        "\n",
        "print(\"Loaded Train Set with Size:\", len(train_data))\n",
        "print(\"Loaded Test Set with Size:\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvwPgpviIfoi"
      },
      "outputs": [],
      "source": [
        "# Shuffling data before saving for better models training and ensuring unbiased model input\n",
        "\n",
        "train_data = train_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "test_data = test_data.sample(frac=1, random_state=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG2vRDEPK6hb"
      },
      "source": [
        "##**Saving the Datasets Splits for the First Scenario**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87G24oFg6Kfy"
      },
      "outputs": [],
      "source": [
        "# Saving Training and Testing splits for the First Scenario\n",
        "train_data_path = '/content/drive/My Drive/#####'\n",
        "test_data_path = '/content/drive/My Drive/#####'\n",
        "\n",
        "\n",
        "train_data.to_csv(train_data_path, index=False)\n",
        "test_data.to_csv(test_data_path, index=False)\n",
        "\n",
        "print(f\"First Scenario Training data saved to {train_data_path}\")\n",
        "print(f\"First Scenario Test data saved to {test_data_path}\")\n",
        "\n",
        "# Load the saved datasets from Google Drive\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "print(\"Loaded First Scenario Training Set with Size:\", len(train_data))\n",
        "print(\"Loaded First Scenario Test Set with Size:\", len(test_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHA9BQVOo5Q8"
      },
      "source": [
        "# **Splitting the Target Data for the Controlled Scenario**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hno0vuJfGpFU"
      },
      "outputs": [],
      "source": [
        "# Reading the CSV files for the chosen 250 most complex reviews as well as the remaining reviews, representing the normal ones\n",
        "\n",
        "drive_destination_file_normal = \"/content/drive/My Drive/#####\"\n",
        "drive_destination_file_complex = \"/content/drive/My Drive/#####\"\n",
        "\n",
        "# Reading the CSV file into a DataFrame\n",
        "preprocessed_df_normal = pd.read_csv(drive_destination_file_normal)\n",
        "preprocessed_df_complex = pd.read_csv(drive_destination_file_complex)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzfKmYtI0AfW"
      },
      "source": [
        "***The following performs the split and verifies the distribution of complex reviews across sectors. It also calculates how many records need to be transferred from normal to complex test set to preserve class distribution***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DVODbSoOewB"
      },
      "outputs": [],
      "source": [
        "# Checking the data splits sizes of the complex data\n",
        "\n",
        "cx_train_data, cx_test_data  = data_split(train_data, 0.7, preprocessed_df_complex['Sector'])\n",
        "\n",
        "print(\"Loaded Complex Train Set with Size:\", len(cx_train_data))\n",
        "print(\"Loaded Complex Test Set with Size:\", len(cx_test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JadY1BjKuciv"
      },
      "outputs": [],
      "source": [
        "# Calculating the number of records that should be in the testing data of this scenario to ensure (80% train+val, 20% test) splitting for the whole data\n",
        "\n",
        "total_records_to_transfer_to_testing = test_data - cx_test_data\n",
        "print(total_records_to_transfer_to_testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWf-DkyEdS_-"
      },
      "outputs": [],
      "source": [
        "# Checking the sector distribution in the test split of the complex data and making sure it is not exceeding that in the test split of the first scenario\n",
        "\n",
        "print(\"Sector Distribution in the Test Split of the Complex Data: \\n\", cx_test_data['Sector'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svn52rMDeVlm"
      },
      "outputs": [],
      "source": [
        "transfer_counts_per_sector = (test_data['Sector'].value_counts() - cx_test_data['Sector'].value_counts()).to_dict()\n",
        "\n",
        "\n",
        "print(\"Specifying the Exact Number of Records to Transfer for each Sector to the Test Data Split in the Second Scenario: \\n\", transfer_counts_per_sector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOfER8Rw0ykp"
      },
      "outputs": [],
      "source": [
        "# Function to transfer a specified number of records by sector independently\n",
        "\n",
        "def transfer_records_by_sector(df1, df2, transfer_counts):\n",
        "    # List to hold records to be transferred\n",
        "    records_to_transfer = []\n",
        "\n",
        "    for sector, count in transfer_counts.items():\n",
        "        # Filter records by sector\n",
        "        sector_df = df2[df2['Sector'] == sector]\n",
        "\n",
        "        # Randomly Select the records to transfer, ensuring we do not transfer more than available records\n",
        "        records = sector_df.sample(n=min(count, len(sector_df)), random_state=1)\n",
        "\n",
        "        # Append to list\n",
        "        records_to_transfer.append(records)\n",
        "\n",
        "    # Concatenate all selected records\n",
        "    records_to_transfer_df = pd.concat(records_to_transfer)\n",
        "\n",
        "    # Remove selected records from df2\n",
        "    df2 = df2.drop(records_to_transfer_df.index)\n",
        "\n",
        "    # Append records to df1\n",
        "    df1 = pd.concat([df1, records_to_transfer_df])\n",
        "\n",
        "    return df1, df2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b2vJJj600ij"
      },
      "source": [
        "Sectors in **cx_test_data** with count exceeding that in the **test_data** split of the first scenario are indicated and extra records have been moved back to the **cx_train_data**. To keep the test size the same, these records were replaced by moving records from the **cx_train_data** to **cx_test_data**, but from other sectors, and distributed proportionally.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***The following is the Pseudocode for preparing the train and test splits for the Second Scenario:***\n",
        "\n",
        "```\n",
        "\n",
        "# Initialize list of sectors with excess records in test\n",
        "Initialize excess_sectors_list = []\n",
        "\n",
        "# Step 1: Identify problem sectors\n",
        "FOR EACH (sector, value) IN transfer_counts_per_sector:\n",
        "\n",
        "    \n",
        "    IF value < 0:\n",
        "        \n",
        "        ADD sector TO excess_sectors_list\n",
        "        \n",
        "        PRINT \"Sector [sector] has a negative count: [value]\"\n",
        "\n",
        "        \n",
        "        # Balance excess in test set\n",
        "        # Move ABS(value) records of this sector from cx_test_data to cx_train_data\n",
        "        Call transfer_records_by_sector to move excess sector records from **cx_test_data** → **cx_train_data** after preparing the correct **transfer_counts** dictionary with ABS(value) no. of records of this sector\n",
        "\n",
        "# Step 2: Maintain test size by refilling from other sectors\n",
        "Calculate total_to_refill = Sum of all ABS(negative values)\n",
        "\n",
        "# Step 3: Get list of eligible sectors\n",
        "eligible_sectors = All unique sectors in cx_train_data EXCLUDING excess_sectors_list\n",
        "\n",
        "# Step 4: Distribute total_to_refill proportionally among eligible_sectors\n",
        "Construct refill_transfer_counts dictionary with only eligible sectors\n",
        "\n",
        "# Step 5: Move records from cx_train_data to cx_test_data\n",
        "Call transfer_records_by_sector to move records from other sectors in **cx_train_data** → **cx_test_data** with the **refill_transfer_counts** dictionary\n",
        "\n",
        "# Step 6: Update the **transfer_counts_per_sector** dictionary based on updated\n",
        "**cx_test_data** (if needed)\n",
        "After the transfers, update or recalculate the sector distribution difference\n",
        "between **cx_test_data** and the reference test_set of the first scenraio to verify balance was achieved.\n",
        "\n",
        "# Step 7: Move remaining records from normal dataset to cx_test_data\n",
        "Call transfer_records_by_sector to transfer records from **normal_data** → **cx_test_data** using the **transfer_counts_per_sector** dictionary to move records from the normal data to cx_test_data to complete the 20% test split.\n",
        "\n",
        "# Step 8: Add remaining records in normal data to cx_train_data to finalize the training set.\n",
        "Append all the remaining **normal_data** to **cx_train_data**\n",
        "(Use pd.concat or call **transfer_records_by_sector** if tracking is needed)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czqo1cOFVSOv"
      },
      "outputs": [],
      "source": [
        "# Checking duplicates to ensure everything was done correctly\n",
        "\n",
        "print(cx_train_data.duplicated().sum())\n",
        "print(cx_test_data.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7LeuSQ6onwa"
      },
      "outputs": [],
      "source": [
        "# Checking the lengths of the updated cx_train_data and cx_train_data splits as well as their sector distriubtion before saving\n",
        "\n",
        "print(\"Size of the Train Set in the Second Scenario:\", len(cx_train_data))\n",
        "print(\"Size of the Test Set in the Second Scenario:\", len(cx_test_data))\n",
        "\n",
        "print(\"Sector Distribution in the Train Split of the Second Scenario: \\n\", cx_train_data['Sector'].value_counts())\n",
        "print(\"Sector Distribution in the Test Split of the Second Scenario: \\n\", cx_test_data['Sector'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfDQQJi8_Nmg"
      },
      "outputs": [],
      "source": [
        "# Shuffling data before saving for better models training and ensuring unbiased model input\n",
        "\n",
        "cx_train_data = cx_train_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "cx_test_data = cx_test_data.sample(frac=1, random_state=1).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeK2utUoGTV0"
      },
      "source": [
        "##**Saving the Datasets Splits for the Second Scenario**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQtD1WYepQZu"
      },
      "outputs": [],
      "source": [
        "# Saving Training and Testing splits for the Second Scenario\n",
        "cx_train_data_path = '/content/drive/My Drive/#####'\n",
        "cx_test_data_path = '/content/drive/My Drive/#####'\n",
        "\n",
        "\n",
        "cx_train_data.to_csv(cx_train_data_path, index=False)\n",
        "cx_test_data.to_csv(cx_test_data_path, index=False)\n",
        "\n",
        "print(f\"Second Scenario Training data saved to {cx_train_data_path}\")\n",
        "print(f\"Second Scenario Test data saved to {cx_test_data_path}\")\n",
        "\n",
        "# Load the saved datasets from Google Drive\n",
        "cx_train_data = pd.read_csv(cx_train_data_path)\n",
        "cx_test_data = pd.read_csv(cx_test_data_path)\n",
        "\n",
        "print(\"Loaded Training Set with Noise Size:\", len(cx_train_data))\n",
        "print(\"Loaded Test Set with Noise Size:\", len(cx_test_data))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
